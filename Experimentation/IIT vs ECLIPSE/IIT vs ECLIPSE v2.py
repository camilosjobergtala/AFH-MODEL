"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
FALSIFICACI√ìN DEFINITIVA DE IIT CON ECLIPSE - VERSI√ìN 2.0 MEJORADA
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Autor: Camilo Alejandro Sj√∂berg Tala + Claude
DOI: 10.5281/zenodo.15541550
Version: 2.0.0-DEFINITIVE
‚úÖ MEJORAS CR√çTICAS:
  - Balanceo de clases (SMOTE + undersampling)
  - Validaci√≥n estratificada
  - Optimizaci√≥n con MCC en lugar de F1
  - 8 canales REALES garantizados
  - M√°s umbrales (200 en lugar de 50)
  - M√©tricas robustas al desbalance
  - An√°lisis de correlaci√≥n Œ¶ vs consciencia
  - M√∫ltiples estrategias de clasificaci√≥n
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

import json
import hashlib
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Callable, Any, Union
from dataclasses import dataclass, asdict, field
import warnings
import sys
from collections import defaultdict
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64
import mne
import os
from scipy import signal
from scipy.stats import entropy, spearmanr, pearsonr
import time
import psutil
import logging

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# GPU SUPPORT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

USE_GPU = False
try:
    import cupy as cp
    import GPUtil
    USE_GPU = True
    print("‚úÖ GPU detectada - Aceleraci√≥n activada")
except ImportError:
    cp = np
    print("‚ö†Ô∏è  GPU no disponible - Usando CPU")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# LOGGING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def setup_logging(output_dir: str):
    log_dir = Path(output_dir) / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"iit_falsification_v2_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return log_file

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# THERMAL MONITOR
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class ThermalMonitor:
    MAX_CPU_TEMP = 85
    MAX_GPU_TEMP = 80
    COOLDOWN_TIME = 60
    CHECK_INTERVAL = 30
    
    def __init__(self):
        self.last_check = 0
        self.cooldown_count = 0
    
    def check_temperature(self, force=False) -> bool:
        current_time = time.time()
        
        if not force and (current_time - self.last_check) < self.CHECK_INTERVAL:
            return True
        
        self.last_check = current_time
        needs_cooldown = False
        
        if USE_GPU:
            try:
                gpus = GPUtil.getGPUs()
                if gpus and len(gpus) > 0:
                    gpu_temp = gpus[0].temperature
                    if gpu_temp > self.MAX_GPU_TEMP:
                        logging.warning(f"üî• GPU caliente: {gpu_temp}¬∞C")
                        needs_cooldown = True
            except:
                pass
        
        try:
            temps = psutil.sensors_temperatures()
            if 'coretemp' in temps:
                max_cpu_temp = max([t.current for t in temps['coretemp']])
                if max_cpu_temp > self.MAX_CPU_TEMP:
                    logging.warning(f"üî• CPU caliente: {max_cpu_temp}¬∞C")
                    needs_cooldown = True
        except:
            pass
        
        if needs_cooldown:
            self.cooldown_count += 1
            logging.info(f"‚è∏Ô∏è  Pausa de enfriamiento #{self.cooldown_count} ({self.COOLDOWN_TIME}s)")
            time.sleep(self.COOLDOWN_TIME)
            return self.check_temperature(force=True)
        
        return True

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ECLIPSE FRAMEWORK
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class FalsificationCriteria:
    name: str
    threshold: float
    comparison: str
    description: str
    is_required: bool = True
    
    def evaluate(self, value: float) -> bool:
        if value is None or np.isnan(value):
            return False
        comparisons = {
            ">=": lambda x, y: x >= y,
            "<=": lambda x, y: x <= y,
            ">": lambda x, y: x > y,
            "<": lambda x, y: x < y,
        }
        return comparisons[self.comparison](value, self.threshold)


@dataclass
class EclipseConfig:
    project_name: str
    researcher: str
    sacred_seed: int
    development_ratio: float = 0.7
    holdout_ratio: float = 0.3
    n_folds_cv: int = 5
    output_dir: str = "./eclipse_results_v2"
    timestamp: str = field(default=None)
    n_channels: int = 8
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()


class EclipseValidator:
    @staticmethod
    def binary_classification_metrics(y_true, y_pred, y_pred_proba=None) -> Dict[str, float]:
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, f1_score,
            confusion_matrix, roc_auc_score, matthews_corrcoef,
            balanced_accuracy_score
        )
        
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1_score': f1_score(y_true, y_pred, zero_division=0),
            'mcc': matthews_corrcoef(y_true, y_pred)
        }
        
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        metrics.update({
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'true_positives': int(tp),
            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0.0,
            'npv': tn / (tn + fn) if (tn + fn) > 0 else 0.0
        })
        
        if y_pred_proba is not None:
            try:
                metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)
            except:
                metrics['roc_auc'] = np.nan
        
        return metrics


class EclipseReporter:
    @staticmethod
    def generate_html_report(final_assessment: Dict, output_path: str = None) -> str:
        project = final_assessment['project_name']
        verdict = final_assessment['verdict']
        val_metrics = final_assessment['validation_summary'].get('metrics', {})
        criteria_eval = final_assessment['criteria_evaluation']
        
        warnings_html = ""
        
        # ROC-AUC warning
        if 'roc_auc' in val_metrics and val_metrics['roc_auc'] < 0.5:
            warnings_html += f'<div style="background:#f8d7da;border-left:4px solid #dc3545;padding:15px;margin:20px 0"><strong>‚ö†Ô∏è ROC-AUC INVERSO ({val_metrics["roc_auc"]:.4f}) - Œ¶ tiene correlaci√≥n NEGATIVA con consciencia</strong></div>'
        
        # MCC warning
        if 'mcc' in val_metrics and abs(val_metrics['mcc']) < 0.1:
            warnings_html += f'<div style="background:#fff3cd;border-left:4px solid #ffc107;padding:15px;margin:20px 0"><strong>‚ö†Ô∏è MCC ‚âà 0 ({val_metrics["mcc"]:.4f}) - Sin capacidad predictiva real</strong></div>'
        
        # Specificity warning
        if 'specificity' in val_metrics and val_metrics['specificity'] < 0.1:
            warnings_html += f'<div style="background:#fff3cd;border-left:4px solid #ffc107;padding:15px;margin:20px 0"><strong>‚ö†Ô∏è SPECIFICITY ‚âà 0 ({val_metrics["specificity"]:.4f}) - Clasificador trivial detectado</strong></div>'
        
        verdict_color = {'VALIDATED': '#28a745', 'FALSIFIED': '#dc3545'}.get(verdict, '#6c757d')
        
        html = f'''<!DOCTYPE html><html><head><meta charset="UTF-8"><title>{project}</title>
<style>body{{font-family:Arial;padding:20px;background:#f5f5f5}}.container{{max-width:1200px;margin:0 auto;background:white;padding:40px}}
.verdict{{background:{verdict_color};color:white;padding:20px;text-align:center;font-size:2em}}table{{width:100%;border-collapse:collapse;margin:20px 0}}
th,td{{padding:12px;text-align:left;border-bottom:1px solid #ddd}}th{{background:#34495e;color:white}}
.pass{{background:#d4edda;color:#155724;padding:5px 10px;border-radius:3px}}.fail{{background:#f8d7da;color:#721c24;padding:5px 10px;border-radius:3px}}
</style></head><body><div class="container"><h1>üî¨ ECLIPSE REPORT v2.0</h1><div class="verdict">{verdict}</div>{warnings_html}
<h2>Criteria Evaluation</h2><table><thead><tr><th>Criterion</th><th>Threshold</th><th>Observed</th><th>Status</th></tr></thead><tbody>'''
        
        for crit in criteria_eval:
            criterion = crit['criterion']
            value = crit.get('value')
            passed = crit.get('passed', False)
            value_str = f"{value:.4f}" if value is not None else "N/A"
            status = "‚úÖ" if passed else "‚ùå"
            status_class = "pass" if passed else "fail"
            html += f'<tr><td>{criterion["name"]}</td><td>{criterion["comparison"]} {criterion["threshold"]}</td><td>{value_str}</td><td><span class="{status_class}">{status}</span></td></tr>'
        
        html += '</tbody></table><h2>Validation Metrics</h2><table><thead><tr><th>Metric</th><th>Value</th></tr></thead><tbody>'
        for k, v in val_metrics.items():
            html += f"<tr><td>{k}</td><td>{v:.4f}</td></tr>"
        html += f'</tbody></table><p>Integrity Hash: <code>{final_assessment.get("final_hash", "")[:32]}...</code></p></div></body></html>'
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(html)
            logging.info(f"‚úÖ HTML: {output_path}")
        return html
    
    @staticmethod
    def generate_text_report(final_assessment: Dict, output_path: str = None) -> str:
        lines = ["=" * 80, "ECLIPSE REPORT v2.0", "=" * 80,
                f"Project: {final_assessment['project_name']}", f"Verdict: {final_assessment['verdict']}", ""]
        
        for crit in final_assessment['criteria_evaluation']:
            criterion = crit['criterion']
            value = crit.get('value')
            passed = crit.get('passed', False)
            value_str = f"{value:.4f}" if value is not None else "N/A"
            lines.append(f"{'‚úÖ' if passed else '‚ùå'} {criterion['name']}: {value_str}")
        
        text = "\n".join(lines)
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(text)
            logging.info(f"‚úÖ Text: {output_path}")
        return text


class EclipseFramework:
    def __init__(self, config: EclipseConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.split_file = self.output_dir / f"{config.project_name}_SPLIT.json"
        self.criteria_file = self.output_dir / f"{config.project_name}_CRITERIA.json"
        self.results_file = self.output_dir / f"{config.project_name}_RESULT.json"
        
        self._split_completed = False
        self._criteria_registered = False
        self._development_completed = False
        self._validation_completed = False
        
        print("=" * 80)
        print("üî¨ ECLIPSE v2.0 INITIALIZED")
        print(f"Project: {config.project_name}")
        print(f"Canales EEG: {config.n_channels}")
        print(f"GPU: {'‚úÖ Activada' if USE_GPU else '‚ùå CPU'}")
        print("=" * 80)
        logging.info(f"ECLIPSE v2.0 inicializado - {config.n_channels} canales")
    
    def stage1_irreversible_split(self, data_identifiers: List[Any], force: bool = False) -> Tuple[List[Any], List[Any]]:
        if self.split_file.exists() and not force:
            with open(self.split_file, 'r', encoding='utf-8') as f:
                split_data = json.load(f)
            self._split_completed = True
            logging.info("Split cargado desde archivo existente")
            return split_data['development_ids'], split_data['holdout_ids']
        
        logging.info("STAGE 1: Creando split irreversible")
        print("\nSTAGE 1: SPLIT")
        np.random.seed(self.config.sacred_seed)
        shuffled_ids = np.array(data_identifiers).copy()
        np.random.shuffle(shuffled_ids)
        
        n_development = int(len(data_identifiers) * self.config.development_ratio)
        development_ids = shuffled_ids[:n_development].tolist()
        holdout_ids = shuffled_ids[n_development:].tolist()
        
        split_data = {
            'project_name': self.config.project_name,
            'split_date': datetime.now().isoformat(),
            'sacred_seed': self.config.sacred_seed,
            'development_ids': development_ids,
            'holdout_ids': holdout_ids,
            'integrity_verification': {
                'split_hash': hashlib.sha256(f"{self.config.sacred_seed}_{sorted(data_identifiers)}".encode()).hexdigest()
            }
        }
        
        with open(self.split_file, 'w', encoding='utf-8') as f:
            json.dump(split_data, f, indent=2)
        
        print(f"‚úÖ {len(development_ids)} dev / {len(holdout_ids)} holdout")
        logging.info(f"Split: {len(development_ids)} dev, {len(holdout_ids)} holdout")
        self._split_completed = True
        return development_ids, holdout_ids
    
    def stage2_register_criteria(self, criteria: List[FalsificationCriteria], force: bool = False) -> Dict:
        if self.criteria_file.exists() and not force:
            with open(self.criteria_file, 'r', encoding='utf-8') as f:
                criteria_data = json.load(f)
            self._criteria_registered = True
            logging.info("Criterios cargados")
            return criteria_data
        
        logging.info("STAGE 2: Registrando criterios")
        print("\nSTAGE 2: CRITERIA")
        criteria_dict = {
            'project_name': self.config.project_name,
            'registration_date': datetime.now().isoformat(),
            'criteria': [asdict(c) for c in criteria],
            'criteria_hash': hashlib.sha256(str([asdict(c) for c in criteria]).encode()).hexdigest()
        }
        
        with open(self.criteria_file, 'w', encoding='utf-8') as f:
            json.dump(criteria_dict, f, indent=2)
        
        print(f"‚úÖ {len(criteria)} criteria registered")
        logging.info(f"{len(criteria)} criterios registrados")
        self._criteria_registered = True
        return criteria_dict
    
    def stage3_development(self, development_data: Any, training_function: Callable, 
                          validation_function: Callable, **kwargs) -> Dict:
        logging.info("STAGE 3: Desarrollo con CV estratificada")
        print("\nSTAGE 3: DEVELOPMENT")
        from sklearn.model_selection import StratifiedKFold
        
        # Extraer labels para estratificaci√≥n
        if isinstance(development_data, pd.DataFrame):
            y_labels = development_data['consciousness'].values
        else:
            y_labels = np.array([d['consciousness'] for d in development_data])
        
        skf = StratifiedKFold(n_splits=self.config.n_folds_cv, shuffle=True, random_state=self.config.sacred_seed)
        cv_results = []
        
        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_labels)), y_labels)):
            print(f"\nFOLD {fold_idx + 1}/{self.config.n_folds_cv}")
            logging.info(f"Fold {fold_idx + 1}/{self.config.n_folds_cv}")
            
            if isinstance(development_data, pd.DataFrame):
                train_data = development_data.iloc[train_idx]
                val_data = development_data.iloc[val_idx]
            else:
                train_data = [development_data[i] for i in train_idx]
                val_data = [development_data[i] for i in val_idx]
            
            try:
                model = training_function(train_data, **kwargs)
                metrics = validation_function(model, val_data, **kwargs)
                cv_results.append({'fold': fold_idx + 1, 'metrics': metrics, 'status': 'success'})
                print(f"   ‚úÖ Complete - MCC: {metrics.get('mcc', 0):.3f}, Bal.Acc: {metrics.get('balanced_accuracy', 0):.3f}")
                logging.info(f"Fold {fold_idx + 1} OK - MCC: {metrics.get('mcc', 0):.3f}")
            except Exception as e:
                print(f"   ‚ùå Failed: {e}")
                logging.error(f"Fold {fold_idx + 1} error: {e}")
                cv_results.append({'fold': fold_idx + 1, 'status': 'failed', 'error': str(e)})
        
        successful_folds = [r for r in cv_results if r['status'] == 'success']
        
        if successful_folds:
            metric_names = list(successful_folds[0]['metrics'].keys())
            aggregated_metrics = {}
            for metric_name in metric_names:
                values = [r['metrics'][metric_name] for r in successful_folds]
                aggregated_metrics[metric_name] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'min': float(np.min(values)),
                    'max': float(np.max(values))
                }
        else:
            aggregated_metrics = {}
        
        self._development_completed = True
        logging.info(f"Desarrollo OK: {len(successful_folds)}/{self.config.n_folds_cv}")
        return {
            'n_folds': self.config.n_folds_cv,
            'n_successful': len(successful_folds),
            'aggregated_metrics': aggregated_metrics
        }
    
    def stage4_single_shot_validation(self, holdout_data: Any, final_model: Any,
                                     validation_function: Callable, force: bool = False, **kwargs) -> Dict:
        if self.results_file.exists() and not force:
            raise RuntimeError("VALIDATION DONE! Use force=True to override")
        
        logging.info("STAGE 4: Validaci√≥n holdout")
        print("\nSTAGE 4: SINGLE-SHOT VALIDATION")
        print("‚ö†Ô∏è  THIS HAPPENS EXACTLY ONCE")
        
        confirmation = input("\nüö® Type 'I ACCEPT SINGLE-SHOT VALIDATION': ")
        
        if confirmation != "I ACCEPT SINGLE-SHOT VALIDATION":
            print("‚ùå Cancelled")
            logging.warning("Validaci√≥n cancelada")
            return None
        
        print("\nüöÄ EXECUTING...")
        logging.info("Ejecutando validaci√≥n...")
        
        try:
            metrics = validation_function(final_model, holdout_data, **kwargs)
            validation_results = {
                'status': 'success',
                'n_holdout_samples': len(holdout_data) if isinstance(holdout_data, (list, pd.DataFrame)) else 0,
                'metrics': {k: float(v) for k, v in metrics.items()},
                'timestamp': datetime.now().isoformat()
            }
            print(f"\n‚úÖ COMPLETE")
            logging.info("Validaci√≥n OK")
        except Exception as e:
            print(f"\n‚ùå FAILED: {e}")
            logging.error(f"Validaci√≥n error: {e}")
            validation_results = {'status': 'failed', 'error': str(e)}
        
        self._validation_completed = True
        return validation_results
    
    def stage5_final_assessment(self, development_results: Dict, validation_results: Dict,
                               generate_reports: bool = True) -> Dict:
        logging.info("STAGE 5: Assessment")
        print("\nSTAGE 5: ASSESSMENT")
        
        with open(self.criteria_file, 'r', encoding='utf-8') as f:
            criteria_data = json.load(f)
        
        criteria_list = [FalsificationCriteria(**c) for c in criteria_data['criteria']]
        holdout_metrics = validation_results.get('metrics', {})
        criteria_evaluation = []
        
        for criterion in criteria_list:
            if criterion.name in holdout_metrics:
                value = holdout_metrics[criterion.name]
                passed = criterion.evaluate(value)
                evaluation = {'criterion': asdict(criterion), 'value': float(value), 'passed': passed}
                print(f"{'‚úÖ' if passed else '‚ùå'} {criterion.name}: {value:.4f} ({criterion.comparison} {criterion.threshold})")
                logging.info(f"{'‚úÖ' if passed else '‚ùå'} {criterion.name}: {value:.4f}")
            else:
                evaluation = {'criterion': asdict(criterion), 'value': None, 'passed': False}
            criteria_evaluation.append(evaluation)
        
        required_criteria = [e for e in criteria_evaluation if e['criterion']['is_required']]
        required_passed = sum(1 for e in required_criteria if e['passed'])
        required_total = len(required_criteria)
        
        verdict = "VALIDATED" if all(e['passed'] for e in required_criteria) else "FALSIFIED"
        
        # Warnings
        if 'roc_auc' in holdout_metrics and holdout_metrics['roc_auc'] < 0.5:
            print(f"\n‚ö†Ô∏è  ROC-AUC < 0.5 ({holdout_metrics['roc_auc']:.4f}): Œ¶ tiene correlaci√≥n NEGATIVA con consciencia")
            logging.warning(f"ROC-AUC inverso: {holdout_metrics['roc_auc']:.4f}")
        
        if 'mcc' in holdout_metrics and abs(holdout_metrics['mcc']) < 0.1:
            print(f"\n‚ö†Ô∏è  MCC ‚âà 0 ({holdout_metrics['mcc']:.4f}): Sin capacidad predictiva real")
            logging.warning(f"MCC ‚âà 0: {holdout_metrics['mcc']:.4f}")
        
        if 'specificity' in holdout_metrics and holdout_metrics['specificity'] < 0.1:
            print(f"\n‚ö†Ô∏è  SPECIFICITY ‚âà 0 ({holdout_metrics['specificity']:.4f}): Clasificador trivial")
            logging.warning(f"Specificity ‚âà 0: {holdout_metrics['specificity']:.4f}")
        
        final_assessment = {
            'project_name': self.config.project_name,
            'researcher': self.config.researcher,
            'assessment_date': datetime.now().isoformat(),
            'configuration': asdict(self.config),
            'development_summary': {
                'n_folds': development_results.get('n_folds'),
                'n_successful': development_results.get('n_successful'),
                'aggregated_metrics': development_results.get('aggregated_metrics', {})
            },
            'validation_summary': {
                'status': validation_results.get('status'),
                'n_holdout_samples': validation_results.get('n_holdout_samples'),
                'metrics': validation_results.get('metrics', {})
            },
            'criteria_evaluation': criteria_evaluation,
            'verdict': verdict,
            'verdict_description': f"{required_passed}/{required_total} required criteria passed",
            'required_criteria_passed': f"{required_passed}/{required_total}"
        }
        
        final_assessment_copy = {k: v for k, v in final_assessment.items()}
        final_assessment['final_hash'] = hashlib.sha256(
            json.dumps(final_assessment_copy, sort_keys=True, default=str).encode()
        ).hexdigest()
        
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(final_assessment, f, indent=2, default=str)
        
        print(f"\n{'‚úÖ' if verdict == 'VALIDATED' else '‚ùå'} VERDICT: {verdict}")
        logging.info(f"VEREDICTO: {verdict}")
        print(f"‚úÖ SAVED: {self.results_file}")
        
        if generate_reports:
            html_path = self.output_dir / f"{self.config.project_name}_REPORT.html"
            EclipseReporter.generate_html_report(final_assessment, str(html_path))
            text_path = self.output_dir / f"{self.config.project_name}_REPORT.txt"
            EclipseReporter.generate_text_report(final_assessment, str(text_path))
        
        return final_assessment
    
    def verify_integrity(self) -> Dict:
        print("\nüîç Verifying integrity...")
        logging.info("Verificando integridad...")
        verification = {'all_valid': True}
        
        if self.results_file.exists():
            with open(self.results_file, 'r', encoding='utf-8') as f:
                results_data = json.load(f)
            stored_hash = results_data.get('final_hash')
            results_copy = {k: v for k, v in results_data.items() if k != 'final_hash'}
            recomputed = hashlib.sha256(json.dumps(results_copy, sort_keys=True, default=str).encode()).hexdigest()
            valid = recomputed == stored_hash
            print(f"{'‚úÖ' if valid else '‚ùå'} Results file")
            if not valid:
                verification['all_valid'] = False
                logging.error("Archivo comprometido")
            else:
                logging.info("Integridad OK")
        
        print(f"{'‚úÖ VALID' if verification['all_valid'] else '‚ùå COMPROMISED'}")
        return verification

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PHI CALCULATION (MEJORADO)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def calculate_phi_star_improved(eeg_segment, use_gpu=USE_GPU):
    """
    Œ¶* mejorado con optimizaciones
    """
    n_channels = eeg_segment.shape[0]
    
    if n_channels > 16:
        logging.warning(f"‚ö†Ô∏è {n_channels} canales > 16, truncando")
        eeg_segment = eeg_segment[:16, :]
        n_channels = 16
    
    if use_gpu and USE_GPU:
        try:
            eeg_gpu = cp.array(eeg_segment)
            thresholds = cp.median(eeg_gpu, axis=1, keepdims=True)
            binary_signals = (eeg_gpu > thresholds).astype(cp.int8)
            
            joint_states = cp.zeros(binary_signals.shape[1], dtype=cp.int32)
            for t in range(binary_signals.shape[1]):
                state = binary_signals[:, t]
                joint_states[t] = cp.sum(state * (2 ** cp.arange(n_channels)))
            
            unique_states, counts = cp.unique(joint_states, return_counts=True)
            probs = counts / len(joint_states)
            H_whole = -cp.sum(probs * cp.log2(probs + 1e-10))
            
            min_mi = float('inf')
            
            from itertools import combinations
            for k in range(1, n_channels):
                for partition_A_indices in combinations(range(n_channels), k):
                    partition_B_indices = tuple(i for i in range(n_channels) if i not in partition_A_indices)
                    
                    states_A = cp.zeros(binary_signals.shape[1], dtype=cp.int32)
                    for t in range(binary_signals.shape[1]):
                        state_A = binary_signals[list(partition_A_indices), t]
                        states_A[t] = cp.sum(state_A * (2 ** cp.arange(len(partition_A_indices))))
                    
                    unique_A, counts_A = cp.unique(states_A, return_counts=True)
                    prob_A = counts_A / len(states_A)
                    H_A = -cp.sum(prob_A * cp.log2(prob_A + 1e-10))
                    
                    states_B = cp.zeros(binary_signals.shape[1], dtype=cp.int32)
                    for t in range(binary_signals.shape[1]):
                        state_B = binary_signals[list(partition_B_indices), t]
                        states_B[t] = cp.sum(state_B * (2 ** cp.arange(len(partition_B_indices))))
                    
                    unique_B, counts_B = cp.unique(states_B, return_counts=True)
                    prob_B = counts_B / len(states_B)
                    H_B = -cp.sum(prob_B * cp.log2(prob_B + 1e-10))
                    
                    MI = float(H_A + H_B - H_whole)
                    
                    if MI < min_mi:
                        min_mi = MI
            
            cp.get_default_memory_pool().free_all_blocks()
            phi = min_mi if min_mi != float('inf') else 0.0
            return max(0.0, float(phi))
            
        except Exception as e:
            logging.warning(f"GPU error, usando CPU: {e}")
    
    # CPU fallback
    binary_signals = np.zeros_like(eeg_segment, dtype=int)
    
    for ch in range(n_channels):
        threshold = np.median(eeg_segment[ch, :])
        binary_signals[ch, :] = (eeg_segment[ch, :] > threshold).astype(int)
    
    joint_states = []
    for t in range(binary_signals.shape[1]):
        state = tuple(binary_signals[:, t])
        joint_states.append(state)
    
    unique_states, counts = np.unique(joint_states, axis=0, return_counts=True)
    probabilities = counts / len(joint_states)
    H_whole = entropy(probabilities, base=2)
    
    min_mi = float('inf')
    
    from itertools import combinations
    for k in range(1, n_channels):
        for partition_A_indices in combinations(range(n_channels), k):
            partition_B_indices = tuple(i for i in range(n_channels) if i not in partition_A_indices)
            
            states_A = []
            for t in range(binary_signals.shape[1]):
                state_A = tuple(binary_signals[list(partition_A_indices), t])
                states_A.append(state_A)
            
            unique_A, counts_A = np.unique(states_A, axis=0, return_counts=True)
            prob_A = counts_A / len(states_A)
            H_A = entropy(prob_A, base=2)
            
            states_B = []
            for t in range(binary_signals.shape[1]):
                state_B = tuple(binary_signals[list(partition_B_indices), t])
                states_B.append(state_B)
            
            unique_B, counts_B = np.unique(states_B, axis=0, return_counts=True)
            prob_B = counts_B / len(states_B)
            H_B = entropy(prob_B, base=2)
            
            MI = H_A + H_B - H_whole
            
            if MI < min_mi:
                min_mi = MI
    
    phi = min_mi if min_mi != float('inf') else 0.0
    return max(0.0, phi)


def load_sleepedf_subject_multichannel(psg_path, hypno_path, n_channels=8, thermal_monitor=None):
    """
    Cargar sujeto garantizando n_channels reales
    """
    if thermal_monitor:
        thermal_monitor.check_temperature()
    
    raw = mne.io.read_raw_edf(psg_path, preload=True, verbose=False)
    
    # Canales EEG prioritarios
    preferred_channels = [
        'EEG Fpz-Cz', 'EEG Pz-Oz',
        'EEG F3-A2', 'EEG F4-A1',
        'EEG C3-A2', 'EEG C4-A1',
        'EEG O1-A2', 'EEG O2-A1',
        'EEG F7-A2', 'EEG F8-A1',
        'EEG T3-A2', 'EEG T4-A1',
        'EEG T5-A2', 'EEG T6-A1',
        'EEG Fz-Cz', 'EEG Cz-Pz'
    ]
    
    available = [ch for ch in preferred_channels if ch in raw.ch_names]
    
    if len(available) < n_channels:
        logging.warning(f"Sujeto con solo {len(available)} canales (< {n_channels})")
        if len(available) < 2:
            return None
        selected = available
    else:
        selected = available[:n_channels]
    
    actual_n_channels = len(selected)
    logging.info(f"Usando {actual_n_channels} canales: {', '.join(selected)}")
    
    raw.pick_channels(selected)
    raw.filter(0.5, 30, fir_design='firwin', verbose=False)
    
    hypno_data = mne.read_annotations(hypno_path)
    
    sfreq = raw.info['sfreq']
    window_size = 30
    n_samples_window = int(window_size * sfreq)
    
    data = raw.get_data()
    n_windows = data.shape[1] // n_samples_window
    
    windows = []
    
    for w in range(n_windows):
        start_sample = w * n_samples_window
        end_sample = start_sample + n_samples_window
        
        eeg_window = data[:, start_sample:end_sample]
        time_center = (start_sample + end_sample) / 2 / sfreq
        
        sleep_stage = None
        for annot in hypno_data:
            if annot['onset'] <= time_center < (annot['onset'] + annot['duration']):
                sleep_stage = annot['description']
                break
        
        if sleep_stage is None:
            continue
        
        consciousness_label = 1 if sleep_stage.startswith('Sleep stage W') else 0
        
        try:
            phi = calculate_phi_star_improved(eeg_window)
        except Exception as e:
            logging.warning(f"Error phi en ventana {w}: {e}")
            phi = 0.0
        
        windows.append({
            'phi': phi,
            'consciousness': consciousness_label,
            'sleep_stage': sleep_stage,
            'window_idx': w,
            'n_channels_used': actual_n_channels
        })
    
    return windows


def buscar_archivos_edf_pares(carpeta_base):
    """B√∫squeda de pares PSG-Hypnogram"""
    pares_encontrados = []
    carpeta_path = Path(carpeta_base)
    
    logging.info(f"Buscando en: {carpeta_base}")
    print(f"\nüîç Buscando archivos EDF en: {carpeta_base}")
    
    archivos_psg = list(carpeta_path.glob("*-PSG.edf"))
    archivos_hypno = list(carpeta_path.glob("*-Hypnogram.edf"))
    
    print(f"üìÇ PSG: {len(archivos_psg)}, Hypnogram: {len(archivos_hypno)}")
    logging.info(f"PSG: {len(archivos_psg)}, Hypno: {len(archivos_hypno)}")
    
    if len(archivos_psg) == 0 or len(archivos_hypno) == 0:
        print("‚ùå No se encontraron archivos")
        logging.error("Sin archivos EDF")
        return []
    
    hypno_map = {}
    for hypno_path in archivos_hypno:
        codigo_hypno = hypno_path.stem.replace("-Hypnogram", "")
        if len(codigo_hypno) >= 7:
            base = codigo_hypno[:-1]
            hypno_map[base] = hypno_path
    
    for psg_path in archivos_psg:
        codigo_psg = psg_path.stem.replace("-PSG", "")
        
        if len(codigo_psg) >= 7 and codigo_psg[-1] == '0':
            base = codigo_psg[:-1]
            
            if base in hypno_map:
                pares_encontrados.append((
                    str(psg_path),
                    str(hypno_map[base]),
                    codigo_psg
                ))
    
    print(f"\n‚úÖ Pares encontrados: {len(pares_encontrados)}")
    logging.info(f"Pares: {len(pares_encontrados)}")
    
    return pares_encontrados


def save_progress(output_dir: Path, subject_data: List, checkpoint_name: str):
    """Guardar checkpoint"""
    checkpoint_file = output_dir / f"{checkpoint_name}.pkl"
    try:
        import pickle
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(subject_data, f)
        logging.info(f"‚úÖ Checkpoint: {checkpoint_file}")
    except Exception as e:
        logging.error(f"Error checkpoint: {e}")


def load_progress(output_dir: Path, checkpoint_name: str):
    """Cargar checkpoint"""
    checkpoint_file = output_dir / f"{checkpoint_name}.pkl"
    if checkpoint_file.exists():
        try:
            import pickle
            with open(checkpoint_file, 'rb') as f:
                data = pickle.load(f)
            logging.info(f"‚úÖ Checkpoint cargado: {checkpoint_file}")
            return data
        except Exception as e:
            logging.error(f"Error cargando: {e}")
            return None
    return None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# BALANCEO Y OPTIMIZACI√ìN MEJORADA
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def balance_dataset(df: pd.DataFrame, method='combined', random_state=2025):
    """
    Balancear dataset con m√∫ltiples estrategias
    
    method:
        - 'undersample': Reducir clase mayoritaria
        - 'oversample': Aumentar clase minoritaria (duplicaci√≥n)
        - 'smote': SMOTE sint√©tico (requiere imblearn)
        - 'combined': Undersample + SMOTE
    """
    from sklearn.utils import resample
    
    df_majority = df[df['consciousness'] == 1]
    df_minority = df[df['consciousness'] == 0]
    
    print(f"\nüîÑ Balanceando dataset ({method}):")
    print(f"   Original - Consciente: {len(df_majority)}, Inconsciente: {len(df_minority)}")
    
    if method == 'undersample':
        df_majority_downsampled = resample(
            df_majority,
            replace=False,
            n_samples=len(df_minority),
            random_state=random_state
        )
        df_balanced = pd.concat([df_majority_downsampled, df_minority])
        
    elif method == 'oversample':
        df_minority_upsampled = resample(
            df_minority,
            replace=True,
            n_samples=len(df_majority),
            random_state=random_state
        )
        df_balanced = pd.concat([df_majority, df_minority_upsampled])
        
    elif method == 'smote':
        try:
            from imblearn.over_sampling import SMOTE
            smote = SMOTE(random_state=random_state)
            X = df[['phi']].values
            y = df['consciousness'].values
            X_resampled, y_resampled = smote.fit_resample(X, y)
            df_balanced = pd.DataFrame({
                'phi': X_resampled.flatten(),
                'consciousness': y_resampled
            })
        except ImportError:
            print("   ‚ö†Ô∏è  imblearn no disponible, usando oversample")
            return balance_dataset(df, method='oversample', random_state=random_state)
            
    elif method == 'combined':
        # Primero undersample de mayor√≠a al doble de minoritaria
        target_majority = len(df_minority) * 2
        df_majority_downsampled = resample(
            df_majority,
            replace=False,
            n_samples=min(target_majority, len(df_majority)),
            random_state=random_state
        )
        df_temp = pd.concat([df_majority_downsampled, df_minority])
        
        # Luego SMOTE para equilibrar completamente
        try:
            from imblearn.over_sampling import SMOTE
            smote = SMOTE(random_state=random_state)
            X = df_temp[['phi']].values
            y = df_temp['consciousness'].values
            X_resampled, y_resampled = smote.fit_resample(X, y)
            df_balanced = pd.DataFrame({
                'phi': X_resampled.flatten(),
                'consciousness': y_resampled
            })
        except ImportError:
            print("   ‚ö†Ô∏è  imblearn no disponible, usando solo undersample")
            df_balanced = df_temp
    
    else:
        df_balanced = df
    
    df_balanced = df_balanced.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    n_conscious = (df_balanced['consciousness'] == 1).sum()
    n_unconscious = (df_balanced['consciousness'] == 0).sum()
    print(f"   Balanceado - Consciente: {n_conscious}, Inconsciente: {n_unconscious}")
    logging.info(f"Balanceo: {n_conscious} consciente, {n_unconscious} inconsciente")
    
    return df_balanced


def optimize_threshold_mcc(train_df: pd.DataFrame, n_thresholds=200):
    """
    Optimizar threshold usando MCC en lugar de F1
    """
    from sklearn.metrics import matthews_corrcoef
    
    phi_min = train_df['phi'].min()
    phi_max = train_df['phi'].max()
    
    best_threshold = None
    best_mcc = -1
    
    thresholds = np.linspace(phi_min, phi_max, n_thresholds)
    
    for threshold in thresholds:
        pred = (train_df['phi'] >= threshold).astype(int)
        true = train_df['consciousness']
        
        mcc = matthews_corrcoef(true, pred)
        
        if mcc > best_mcc:
            best_mcc = mcc
            best_threshold = threshold
    
    return {'phi_threshold': best_threshold, 'best_mcc_train': best_mcc}


def analyze_phi_correlation(df: pd.DataFrame, output_dir: Path):
    """
    Analizar correlaci√≥n entre Œ¶ y consciencia
    """
    print("\nüìä An√°lisis de Correlaci√≥n Œ¶ vs Consciencia:")
    
    conscious = df[df['consciousness'] == 1]['phi']
    unconscious = df[df['consciousness'] == 0]['phi']
    
    # Estad√≠sticas descriptivas
    print(f"\n   Œ¶ en VIGILIA (consciente):")
    print(f"      Media: {conscious.mean():.4f} ¬± {conscious.std():.4f}")
    print(f"      Mediana: {conscious.median():.4f}")
    print(f"      Rango: [{conscious.min():.4f}, {conscious.max():.4f}]")
    
    print(f"\n   Œ¶ en SUE√ëO (inconsciente):")
    print(f"      Media: {unconscious.mean():.4f} ¬± {unconscious.std():.4f}")
    print(f"      Mediana: {unconscious.median():.4f}")
    print(f"      Rango: [{unconscious.min():.4f}, {unconscious.max():.4f}]")
    
    # Test estad√≠stico
    from scipy.stats import mannwhitneyu, ttest_ind
    
    u_stat, p_value_mw = mannwhitneyu(conscious, unconscious, alternative='two-sided')
    t_stat, p_value_t = ttest_ind(conscious, unconscious)
    
    print(f"\n   Mann-Whitney U test: U={u_stat:.0f}, p={p_value_mw:.6f}")
    print(f"   T-test independiente: t={t_stat:.4f}, p={p_value_t:.6f}")
    
    # Correlaciones
    pearson_r, pearson_p = pearsonr(df['phi'], df['consciousness'])
    spearman_r, spearman_p = spearmanr(df['phi'], df['consciousness'])
    
    print(f"\n   Correlaci√≥n de Pearson: r={pearson_r:.4f}, p={pearson_p:.6f}")
    print(f"   Correlaci√≥n de Spearman: œÅ={spearman_r:.4f}, p={spearman_p:.6f}")
    
    # Interpretaci√≥n
    if pearson_r < 0:
        print(f"\n   ‚ö†Ô∏è  CORRELACI√ìN NEGATIVA: Œ¶ DISMINUYE con consciencia (opuesto a IIT)")
    elif pearson_r < 0.1:
        print(f"\n   ‚ö†Ô∏è  CORRELACI√ìN MUY D√âBIL: Œ¶ casi no predice consciencia")
    elif pearson_r < 0.3:
        print(f"\n   ‚ö†Ô∏è  CORRELACI√ìN D√âBIL: Œ¶ predice poco la consciencia")
    else:
        print(f"\n   ‚úÖ CORRELACI√ìN POSITIVA: Œ¶ aumenta con consciencia (consistente con IIT)")
    
    # Guardar an√°lisis
    correlation_analysis = {
        'conscious_phi_mean': float(conscious.mean()),
        'conscious_phi_std': float(conscious.std()),
        'conscious_phi_median': float(conscious.median()),
        'unconscious_phi_mean': float(unconscious.mean()),
        'unconscious_phi_std': float(unconscious.std()),
        'unconscious_phi_median': float(unconscious.median()),
        'mannwhitney_u': float(u_stat),
        'mannwhitney_p': float(p_value_mw),
        'ttest_t': float(t_stat),
        'ttest_p': float(p_value_t),
        'pearson_r': float(pearson_r),
        'pearson_p': float(pearson_p),
        'spearman_rho': float(spearman_r),
        'spearman_p': float(spearman_p)
    }
    
    analysis_file = output_dir / "phi_correlation_analysis.json"
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(correlation_analysis, f, indent=2)
    
    logging.info(f"An√°lisis de correlaci√≥n guardado: {analysis_file}")
    
    return correlation_analysis


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN MEJORADO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    print("=" * 80)
    print("üß† FALSIFICACI√ìN DEFINITIVA DE IIT - VERSI√ìN 2.0")
    print("   ‚úÖ Balanceo de clases")
    print("   ‚úÖ Optimizaci√≥n MCC")
    print("   ‚úÖ Validaci√≥n estratificada")
    print("   ‚úÖ An√°lisis de correlaci√≥n")
    print("=" * 80)
    
    # Setup
    log_file = setup_logging("./eclipse_results_v2")
    print(f"\nüìù Log: {log_file}")
    logging.info("=" * 80)
    logging.info("INICIO - VERSI√ìN 2.0 DEFINITIVA")
    logging.info("=" * 80)
    
    sleep_edf_path = input("\nRuta Sleep-EDF: ").strip().strip('"').strip("'")
    
    if not os.path.exists(sleep_edf_path):
        print(f"‚ùå Ruta no existe: {sleep_edf_path}")
        logging.error(f"Ruta inv√°lida: {sleep_edf_path}")
        return
    
    limit = input("¬øLimitar sujetos? (Enter=todos): ").strip()
    limit_n = int(limit) if limit else None
    
    n_channels = input("¬øCanales EEG? (2-16, recomendado 8): ").strip()
    n_channels = int(n_channels) if n_channels else 8
    n_channels = max(2, min(16, n_channels))
    
    balance_method = input("M√©todo balanceo (undersample/oversample/smote/combined): ").strip().lower()
    if balance_method not in ['undersample', 'oversample', 'smote', 'combined']:
        balance_method = 'combined'
    
    print(f"\n‚öôÔ∏è  Configuraci√≥n:")
    print(f"   - Canales EEG: {n_channels}")
    print(f"   - GPU: {'‚úÖ' if USE_GPU else '‚ùå'}")
    print(f"   - L√≠mite: {limit_n if limit_n else 'Todos'}")
    print(f"   - Balanceo: {balance_method}")
    logging.info(f"Config: {n_channels}ch, GPU={USE_GPU}, l√≠mite={limit_n}, balance={balance_method}")
    
    thermal_monitor = ThermalMonitor()
    
    print("\nüöÄ Buscando archivos...")
    subject_pairs = buscar_archivos_edf_pares(sleep_edf_path)
    
    if len(subject_pairs) == 0:
        print("\n‚ùå Sin pares v√°lidos")
        logging.error("Sin pares")
        return
    
    print(f"\n‚úÖ {len(subject_pairs)} pares encontrados")
    
    if limit_n:
        subject_pairs = subject_pairs[:limit_n]
        print(f"   üìä Limitando a {len(subject_pairs)}")
        logging.info(f"Limitando a {len(subject_pairs)}")
    
    subject_ids = [pair[2] for pair in subject_pairs]
    
    output_dir = Path("./eclipse_results_v2")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_name = f"processing_v2_{n_channels}ch_{balance_method}"
    all_windows = load_progress(output_dir, checkpoint_name)
    
    if all_windows is not None:
        print(f"\n‚ôªÔ∏è  Checkpoint: {len(all_windows)} ventanas")
        resume = input("¬øContinuar? (s/n): ").strip().lower()
        if resume != 's':
            all_windows = []
    else:
        all_windows = []
    
    processed_count = len([w for w in all_windows if 'subject_id' in w])
    
    print(f"\nüîÑ Procesando {len(subject_pairs)} sujetos...")
    logging.info(f"Procesando {len(subject_pairs)} sujetos")
    
    start_time = time.time()
    
    for i, (psg, hypno, subject_id) in enumerate(subject_pairs, 1):
        if i <= processed_count:
            continue
        
        print(f"\nüìä {i}/{len(subject_pairs)}: {subject_id}")
        logging.info(f"Procesando {i}/{len(subject_pairs)}: {subject_id}")
        
        if not thermal_monitor.check_temperature():
            logging.warning("Enfriamiento requerido")
        
        subject_start = time.time()
        
        try:
            windows = load_sleepedf_subject_multichannel(psg, hypno, n_channels, thermal_monitor)
            
            if windows and len(windows) > 0:
                for w in windows:
                    w['subject_id'] = subject_id
                
                all_windows.extend(windows)
                print(f"   ‚úÖ {len(windows)} ventanas")
                
                subject_time = time.time() - subject_start
                print(f"   ‚è±Ô∏è  {subject_time:.1f}s")
                logging.info(f"{subject_id}: {len(windows)} ventanas, {subject_time:.1f}s")
                
                if i % 10 == 0:
                    save_progress(output_dir, all_windows, checkpoint_name)
                    print(f"   üíæ Checkpoint ({i}/{len(subject_pairs)})")
                    
                    elapsed = time.time() - start_time
                    avg_time = elapsed / i
                    remaining = (len(subject_pairs) - i) * avg_time
                    eta = datetime.now() + pd.Timedelta(seconds=remaining)
                    print(f"   üìà ETA: {eta.strftime('%H:%M:%S')}")
                    logging.info(f"ETA: {eta}")
            else:
                print(f"   ‚ö†Ô∏è  Sin datos")
                logging.warning(f"{subject_id} sin datos")
                
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            logging.error(f"Error {subject_id}: {e}")
            continue
    
    save_progress(output_dir, all_windows, checkpoint_name)
    
    if len(all_windows) == 0:
        print("\n‚ùå Sin datos v√°lidos")
        logging.error("Sin datos")
        return
    
    df = pd.DataFrame(all_windows)
    
    total_time = time.time() - start_time
    
    print("\n" + "=" * 80)
    print("‚úÖ DATASET COMPLETO")
    print("=" * 80)
    print(f"Ventanas: {len(df)}")
    print(f"Sujetos: {len(subject_ids)}")
    print(f"Conscientes: {(df['consciousness'] == 1).sum()} ({(df['consciousness'] == 1).sum()/len(df)*100:.1f}%)")
    print(f"Inconscientes: {(df['consciousness'] == 0).sum()} ({(df['consciousness'] == 0).sum()/len(df)*100:.1f}%)")
    print(f"Œ¶: {df['phi'].mean():.4f} ¬± {df['phi'].std():.4f}")
    print(f"Rango Œ¶: [{df['phi'].min():.4f}, {df['phi'].max():.4f}]")
    print(f"‚è±Ô∏è  {total_time/60:.1f} min")
    print(f"üî• Pausas: {thermal_monitor.cooldown_count}")
    
    logging.info("=" * 80)
    logging.info(f"DATASET: {len(df)} ventanas, {len(subject_ids)} sujetos")
    logging.info(f"Œ¶: {df['phi'].mean():.4f} ¬± {df['phi'].std():.4f}")
    logging.info(f"Tiempo: {total_time/60:.1f} min")
    logging.info("=" * 80)
    
    # An√°lisis de correlaci√≥n PRE-balanceo
    print("\n" + "=" * 80)
    print("üìä AN√ÅLISIS PRE-BALANCEO")
    print("=" * 80)
    analyze_phi_correlation(df, output_dir)
    
    # ECLIPSE
    config = EclipseConfig(
        project_name=f"IIT_Falsification_v2_{n_channels}ch_{balance_method}",
        researcher="Camilo Alejandro Sj√∂berg Tala",
        sacred_seed=2025,
        output_dir=str(output_dir),
        n_channels=n_channels
    )
    
    eclipse = EclipseFramework(config)
    
    # Stage 1: Split
    print("\n" + "=" * 80)
    print("STAGE 1: SPLIT")
    print("=" * 80)
    dev_subjects, holdout_subjects = eclipse.stage1_irreversible_split(subject_ids)
    
    # Split en dataframe
    dev_mask = df['subject_id'].isin(dev_subjects)
    dev_data = df[dev_mask].reset_index(drop=True)
    holdout_data = df[~dev_mask].reset_index(drop=True)
    
    print(f"\nüìä Divisi√≥n:")
    print(f"   Dev: {len(dev_data)} ventanas")
    print(f"   Holdout: {len(holdout_data)} ventanas")
    
    # BALANCEO solo en desarrollo
    print("\n" + "=" * 80)
    print("‚öñÔ∏è  BALANCEO DE DESARROLLO")
    print("=" * 80)
    dev_data_balanced = balance_dataset(dev_data, method=balance_method)
    
    # Stage 2: Criterios MEJORADOS
    print("\n" + "=" * 80)
    print("STAGE 2: CRITERIOS (MEJORADOS)")
    print("=" * 80)
    
    # Criterios m√°s estrictos y robustos
    criteria = [
        # Criterios principales (robustos al desbalance)
        FalsificationCriteria("balanced_accuracy", 0.65, ">=", "Balanced Accuracy ‚â• 0.65", True),
        FalsificationCriteria("mcc", 0.30, ">=", "MCC ‚â• 0.30 (correlaci√≥n moderada)", True),
        FalsificationCriteria("specificity", 0.50, ">=", "Specificity ‚â• 0.50 (detecta sue√±o)", True),
        
        # Criterios secundarios
        FalsificationCriteria("precision", 0.60, ">=", "Precision ‚â• 0.60", False),
        FalsificationCriteria("recall", 0.60, ">=", "Recall ‚â• 0.60", False),
        FalsificationCriteria("roc_auc", 0.65, ">=", "ROC-AUC ‚â• 0.65", False),
        FalsificationCriteria("f1_score", 0.60, ">=", "F1 Score ‚â• 0.60", False)
    ]
    
    eclipse.stage2_register_criteria(criteria)
    
    print("\nüìã Criterios registrados:")
    print("   REQUERIDOS (deben pasar todos):")
    print("      ‚úì Balanced Accuracy ‚â• 0.65")
    print("      ‚úì MCC ‚â• 0.30")
    print("      ‚úì Specificity ‚â• 0.50")
    print("   OPCIONALES (informativos):")
    print("      ‚óã Precision, Recall, ROC-AUC, F1")
    
    # Stage 3: Desarrollo con MCC
    print("\n" + "=" * 80)
    print("STAGE 3: DESARROLLO (OPTIMIZACI√ìN MCC)")
    print("=" * 80)
    
    def train_iit_improved(train_data, **kwargs):
        """Training mejorado con optimizaci√≥n MCC"""
        if isinstance(train_data, pd.DataFrame):
            train_df = train_data
        else:
            train_df = dev_data_balanced.iloc[train_data]
        
        # Optimizar con MCC
        model = optimize_threshold_mcc(train_df, n_thresholds=200)
        
        return model
    
    def validate_iit_improved(model, val_data, **kwargs):
        """Validaci√≥n mejorada"""
        if isinstance(val_data, pd.DataFrame):
            val_df = val_data
        else:
            val_df = dev_data_balanced.iloc[val_data]
        
        threshold = model['phi_threshold']
        y_pred = (val_df['phi'] >= threshold).astype(int)
        y_true = val_df['consciousness']
        
        # Normalizaci√≥n correcta para ROC-AUC
        phi_min = val_df['phi'].min()
        phi_max = val_df['phi'].max()
        if phi_max > phi_min:
            y_pred_proba = (val_df['phi'] - phi_min) / (phi_max - phi_min + 1e-10)
        else:
            y_pred_proba = np.ones(len(val_df)) * 0.5
        
        return EclipseValidator.binary_classification_metrics(y_true, y_pred, y_pred_proba)
    
    dev_results = eclipse.stage3_development(
        development_data=dev_data_balanced,
        training_function=train_iit_improved,
        validation_function=validate_iit_improved
    )
    
    print(f"\nüìà Resultados CV (m√©tricas clave):")
    key_metrics = ['balanced_accuracy', 'mcc', 'specificity', 'roc_auc']
    for metric_name in key_metrics:
        if metric_name in dev_results['aggregated_metrics']:
            stats = dev_results['aggregated_metrics'][metric_name]
            print(f"   {metric_name}: {stats['mean']:.3f} ¬± {stats['std']:.3f}")
    
    # Stage 4: Validaci√≥n Final
    print("\n" + "=" * 80)
    print("STAGE 4: VALIDACI√ìN HOLDOUT")
    print("=" * 80)
    print("üîß Entrenando modelo final...")
    
    # Entrenar con TODO el conjunto de desarrollo balanceado
    final_model = train_iit_improved(dev_data_balanced)
    print(f"   Œ¶ threshold: {final_model['phi_threshold']:.4f}")
    print(f"   MCC training: {final_model['best_mcc_train']:.4f}")
    
    def validate_final_improved(model, holdout_df, **kwargs):
        """Validaci√≥n final mejorada"""
        threshold = model['phi_threshold']
        y_pred = (holdout_df['phi'] >= threshold).astype(int)
        y_true = holdout_df['consciousness']
        
        # Normalizaci√≥n correcta
        phi_min = holdout_df['phi'].min()
        phi_max = holdout_df['phi'].max()
        if phi_max > phi_min:
            y_pred_proba = (holdout_df['phi'] - phi_min) / (phi_max - phi_min + 1e-10)
        else:
            y_pred_proba = np.ones(len(holdout_df)) * 0.5
        
        return EclipseValidator.binary_classification_metrics(y_true, y_pred, y_pred_proba)
    
    val_results = eclipse.stage4_single_shot_validation(
        holdout_data=holdout_data,
        final_model=final_model,
        validation_function=validate_final_improved
    )
    
    if val_results is None:
        print("\n‚ö†Ô∏è  Validaci√≥n cancelada")
        logging.warning("Validaci√≥n cancelada")
        return
    
    # An√°lisis de correlaci√≥n en holdout
    print("\nüìä An√°lisis Holdout:")
    analyze_phi_correlation(holdout_data, output_dir)
    
    # Stage 5: Assessment
    print("\n" + "=" * 80)
    print("STAGE 5: ASSESSMENT FINAL")
    print("=" * 80)
    final_assessment = eclipse.stage5_final_assessment(dev_results, val_results, generate_reports=True)
    
    # Resumen Final Extendido
    print("\n" + "=" * 80)
    print("üéØ RESULTADOS FINALES - IIT FALSIFICATION v2.0")
    print("=" * 80)
    print(f"\n{'‚úÖ' if final_assessment['verdict'] == 'VALIDATED' else '‚ùå'} VEREDICTO: {final_assessment['verdict']}")
    
    print("\nüìä M√âTRICAS HOLDOUT (SIN BALANCEO):")
    holdout_metrics = final_assessment['validation_summary']['metrics']
    
    critical_metrics = ['balanced_accuracy', 'mcc', 'specificity', 'roc_auc']
    print("\n   Cr√≠ticas:")
    for k in critical_metrics:
        if k in holdout_metrics:
            print(f"      {k}: {holdout_metrics[k]:.4f}")
    
    print("\n   Adicionales:")
    other_metrics = ['accuracy', 'precision', 'recall', 'f1_score']
    for k in other_metrics:
        if k in holdout_metrics:
            print(f"      {k}: {holdout_metrics[k]:.4f}")
    
    print("\n   Matriz de Confusi√≥n:")
    print(f"      TN: {holdout_metrics.get('true_negatives', 0):.0f} | FP: {holdout_metrics.get('false_positives', 0):.0f}")
    print(f"      FN: {holdout_metrics.get('false_negatives', 0):.0f} | TP: {holdout_metrics.get('true_positives', 0):.0f}")
    
    # Interpretaci√≥n cient√≠fica
    print("\n" + "=" * 80)
    print("üî¨ INTERPRETACI√ìN CIENT√çFICA")
    print("=" * 80)
    
    mcc = holdout_metrics.get('mcc', 0)
    roc_auc = holdout_metrics.get('roc_auc', 0.5)
    specificity = holdout_metrics.get('specificity', 0)
    bal_acc = holdout_metrics.get('balanced_accuracy', 0.5)
    
    if mcc < 0:
        print("‚ùå MCC NEGATIVO: Œ¶ predice en direcci√≥n OPUESTA a consciencia")
        print("   ‚Üí IIT FALSIFICADA: La teor√≠a predice lo contrario de lo observado")
    elif mcc < 0.1:
        print("‚ùå MCC ‚âà 0: Œ¶ NO tiene relaci√≥n con consciencia")
        print("   ‚Üí IIT FALSIFICADA: Sin capacidad predictiva")
    elif mcc < 0.3:
        print("‚ö†Ô∏è  MCC D√âBIL: Œ¶ tiene correlaci√≥n muy baja con consciencia")
        print("   ‚Üí IIT CUESTIONADA: Evidencia d√©bil a favor")
    elif mcc < 0.5:
        print("‚ö†Ô∏è  MCC MODERADO: Œ¶ tiene correlaci√≥n moderada con consciencia")
        print("   ‚Üí IIT PARCIALMENTE APOYADA: Evidencia moderada")
    else:
        print("‚úÖ MCC FUERTE: Œ¶ tiene correlaci√≥n fuerte con consciencia")
        print("   ‚Üí IIT APOYADA: Evidencia fuerte a favor")
    
    if roc_auc < 0.5:
        print(f"\n‚ùå ROC-AUC < 0.5 ({roc_auc:.3f}): Clasificador INVERSO")
        print("   ‚Üí Œ¶ tiene relaci√≥n NEGATIVA con consciencia")
    elif roc_auc < 0.6:
        print(f"\n‚ö†Ô∏è  ROC-AUC bajo ({roc_auc:.3f}): Apenas mejor que azar")
    elif roc_auc < 0.7:
        print(f"\n‚ö†Ô∏è  ROC-AUC moderado ({roc_auc:.3f}): Discriminaci√≥n aceptable")
    else:
        print(f"\n‚úÖ ROC-AUC bueno ({roc_auc:.3f}): Buena discriminaci√≥n")
    
    if specificity < 0.1:
        print(f"\n‚ùå SPECIFICITY ‚âà 0 ({specificity:.3f}): Clasificador TRIVIAL")
        print("   ‚Üí El modelo predice siempre 'consciente' (ignora Œ¶)")
    elif specificity < 0.5:
        print(f"\n‚ö†Ô∏è  SPECIFICITY baja ({specificity:.3f}): Detecta mal el sue√±o")
    else:
        print(f"\n‚úÖ SPECIFICITY adecuada ({specificity:.3f}): Detecta bien el sue√±o")
    
    # Guardar an√°lisis detallado
    detailed_analysis = {
        'verdict': final_assessment['verdict'],
        'dataset_info': {
            'total_windows': len(df),
            'n_subjects': len(subject_ids),
            'n_channels': n_channels,
            'balance_method': balance_method,
            'conscious_windows': int((df['consciousness'] == 1).sum()),
            'unconscious_windows': int((df['consciousness'] == 0).sum())
        },
        'phi_statistics': {
            'overall_mean': float(df['phi'].mean()),
            'overall_std': float(df['phi'].std()),
            'conscious_mean': float(df[df['consciousness'] == 1]['phi'].mean()),
            'unconscious_mean': float(df[df['consciousness'] == 0]['phi'].mean())
        },
        'holdout_metrics': holdout_metrics,
        'interpretation': {
            'mcc_interpretation': 'negative' if mcc < 0 else 'none' if mcc < 0.1 else 'weak' if mcc < 0.3 else 'moderate' if mcc < 0.5 else 'strong',
            'roc_auc_interpretation': 'inverse' if roc_auc < 0.5 else 'poor' if roc_auc < 0.6 else 'fair' if roc_auc < 0.7 else 'good',
            'specificity_interpretation': 'trivial' if specificity < 0.1 else 'poor' if specificity < 0.5 else 'adequate',
            'final_conclusion': 'FALSIFIED' if mcc < 0.3 or specificity < 0.5 else 'QUESTIONABLE' if mcc < 0.5 else 'SUPPORTED'
        }
    }
    
    analysis_file = output_dir / f"{config.project_name}_DETAILED_ANALYSIS.json"
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_analysis, f, indent=2)
    
    print(f"\nüìÅ Resultados guardados en: {output_dir}")
    print(f"   - Reporte HTML: {config.project_name}_REPORT.html")
    print(f"   - Reporte texto: {config.project_name}_REPORT.txt")
    print(f"   - JSON completo: {config.project_name}_RESULT.json")
    print(f"   - An√°lisis detallado: {config.project_name}_DETAILED_ANALYSIS.json")
    print(f"   - Correlaciones: phi_correlation_analysis.json")
    print(f"   - Log: {log_file}")
    
    # Verificaci√≥n de integridad
    eclipse.verify_integrity()
    
    print("\n" + "=" * 80)
    print("‚úÖ ECLIPSE v2.0 COMPLETADO")
    print(f"‚è±Ô∏è  Tiempo total: {(time.time() - start_time)/3600:.2f} horas")
    print("=" * 80)
    
    # Recomendaciones finales
    print("\nüìö RECOMENDACIONES PARA PUBLICACI√ìN:")
    print()
    if final_assessment['verdict'] == 'FALSIFIED':
        print("1. T√≠tulo sugerido:")
        print('   "Empirical Test of Integrated Information Theory using')
        print(f'    EEG Data: {n_channels}-Channel Analysis with Rigorous Pre-registration"')
        print()
        print("2. Conclusi√≥n principal:")
        print(f'   "Una aproximaci√≥n computacionalmente factible de Œ¶* con {n_channels}')
        print(f'    canales EEG fall√≥ en distinguir estados de vigilia de sue√±o seg√∫n')
        print(f'    criterios pre-registrados (MCC={mcc:.3f}, Bal.Acc={bal_acc:.3f})."')
        print()
        print("3. Limitaciones a reportar:")
        print(f"   - Œ¶* es una aproximaci√≥n simplificada de Œ¶ completo (IIT 3.0/4.0)")
        print(f"   - Solo {n_channels} canales EEG (limitaci√≥n computacional)")
        print(f"   - Contraste vigilia/sue√±o (no anestesia/coma)")
        print(f"   - Binarizaci√≥n de se√±ales es simplificaci√≥n")
    else:
        print("1. T√≠tulo sugerido:")
        print('   "Partial Support for Integrated Information Theory: ')
        print(f'    {n_channels}-Channel EEG Analysis with Pre-registered Criteria"')
        print()
        print("2. Conclusi√≥n principal:")
        print(f'   "Una aproximaci√≥n de Œ¶* con {n_channels} canales mostr√≥ capacidad')
        print(f'    moderada para distinguir vigilia de sue√±o (MCC={mcc:.3f}),')
        print(f'    proporcionando apoyo parcial a predicciones de IIT."')
    
    print("\n4. Mencionar siempre:")
    print("   ‚úì Framework ECLIPSE usado para integridad metodol√≥gica")
    print("   ‚úì Pre-registro de criterios antes de validaci√≥n")
    print("   ‚úì Split irreversible desarrollo/holdout")
    print("   ‚úì Validaci√≥n √∫nica (single-shot)")
    print("   ‚úì Balanceo de clases y m√©tricas robustas al desbalance")
    
    logging.info("=" * 80)
    logging.info("EJECUCI√ìN COMPLETADA")
    logging.info(f"Tiempo: {(time.time() - start_time)/3600:.2f} horas")
    logging.info(f"Veredicto: {final_assessment['verdict']}")
    logging.info("=" * 80)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrumpido por usuario")
        logging.warning("Interrumpido")
    except Exception as e:
        print(f"\n\n‚ùå Error cr√≠tico: {e}")
        logging.error(f"Error cr√≠tico: {e}")
        import traceback
        logging.error(traceback.format_exc())