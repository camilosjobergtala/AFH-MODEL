import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=UserWarning)

import threading
import time
import os
import re
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from sklearn.model_selection import KFold, StratifiedKFold
import multiprocessing
import random
from sklearn.metrics import mutual_info_score, precision_score, recall_score, f1_score
import mne
from tqdm import tqdm
import networkx as nx
from typing import Dict, List, Tuple, Optional
import logging
from scipy import stats
from scipy.signal import find_peaks
from sklearn.cluster import KMeans
import json
import pickle
import hashlib
from pathlib import Path
import psutil
import gc

# ===============================================
# üöÄ CONFIGURACI√ìN OPTIMIZADA PARA i7-11800H
# ===============================================

# Hardware espec√≠fico
CPU_CORES = 8  # i7-11800H f√≠sica
CPU_THREADS = 16  # Hyper-threading
RAM_GB = 32
OPTIMAL_WORKERS = min(12, CPU_THREADS - 2)  # Dejar 4 threads para sistema
CHUNK_SIZE = 4  # Procesar en lotes peque√±os para evitar saturar RAM
# Configuraci√≥n del pipeline
TAMANO_VENTANA = 30  # segundos
CARPETA_BASE = r"G:\Mi unidad\AFH\EXPERIMENTO\FASE 2\SLEEP-EDF\DATABASE\sleep-cassette"
CARPETA_RESULTADOS = os.path.join(CARPETA_BASE, "EUREKA_OPTIMIZED_i7")
VENTANAS_ANTES = 5
VENTANA_MARGIN = 3
VENTANA_MARGIN_LEVE = 5
MARGEN_DELTA_PCI = 0.1

# ===============================================
# üéØ UMBRALES PAH* V2.1 CALIBRADOS
# ===============================================

K_TOPO_THRESHOLD = 0.300     # Calibrado emp√≠ricamente - F1=0.415
PHI_H_THRESHOLD = 0.400      # Optimizado anti-overfitting
DELTA_PCI_THRESHOLD = 0.150  # Umbral robusto con generalizaci√≥n

# Configuraci√≥n de checkpoint y progreso
CHECKPOINT_DIR = os.path.join(CARPETA_RESULTADOS, "checkpoints")
PROGRESS_FILE = os.path.join(CARPETA_RESULTADOS, "progress.json")
BATCH_SIZE = 1  # Procesar de a 1 archivo para checkpoints frecuentes

# Suprimir warnings de MNE
mne.set_log_level('WARNING')
# ===============================================
# üîß SISTEMA DE CHECKPOINT Y PROGRESO
# ===============================================

class ProgressManager:
    """Gestor de progreso con persistencia para recuperaci√≥n autom√°tica"""
    
    def __init__(self, progress_file: str):
        self.progress_file = progress_file
        self.data = self._load_progress()
    
    def _load_progress(self):
        """Carga progreso existente o inicializa nuevo"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {
            'processed_files': [],
            'failed_files': [],
            'total_files': 0,
            'current_batch': 0,
            'start_time': time.time(),
            'last_checkpoint': None,
            'stats': {
                'total_singularities': 0,
                'total_transitions': 0,
                'processing_time': 0
            }
        }
    def save_progress(self):
        """Guarda progreso actual"""
        safe_makedirs(os.path.dirname(self.progress_file))
        with open(self.progress_file, 'w') as f:
            json.dump(self.data, f, indent=2, default=str)
    
    def is_file_processed(self, filename: str) -> bool:
        """Verifica si un archivo ya fue procesado"""
        return filename in self.data['processed_files']
    
    def mark_file_processed(self, filename: str, success: bool = True):
        """Marca archivo como procesado"""
        if success:
            if filename not in self.data['processed_files']:
                self.data['processed_files'].append(filename)
        else:
            if filename not in self.data['failed_files']:
                self.data['failed_files'].append(filename)
        self.save_progress()
    
    def update_stats(self, singularities: int, transitions: int, processing_time: float):
        """Actualiza estad√≠sticas"""
        self.data['stats']['total_singularities'] += singularities
        self.data['stats']['total_transitions'] += transitions
        self.data['stats']['processing_time'] += processing_time
        self.data['last_checkpoint'] = time.time()
        self.save_progress()

    def get_progress_percentage(self) -> float:
        """Calcula porcentaje de progreso"""
        if self.data['total_files'] == 0:
            return 0.0
        processed = len(self.data['processed_files'])
        return (processed / self.data['total_files']) * 100
    
    def get_eta(self) -> str:
        """Estima tiempo restante"""
        processed = len(self.data['processed_files'])
        if processed == 0 or self.data['total_files'] == 0:
            return "Calculando..."
        
        elapsed = time.time() - self.data['start_time']
        rate = processed / elapsed
        remaining = self.data['total_files'] - processed
        eta_seconds = remaining / rate if rate > 0 else float('inf')
        
        if eta_seconds == float('inf'):
            return "‚àû"
        
        hours = int(eta_seconds // 3600)
        minutes = int((eta_seconds % 3600) // 60)
        return f"{hours:02d}h {minutes:02d}m"
    # ===============================================
# üîß LOGGING OPTIMIZADO CON PROGRESO
# ===============================================

def setup_optimized_logging():
    """Configura logging optimizado para monitoreo de progreso"""
    safe_makedirs(CARPETA_RESULTADOS)
    
    # Logger principal
    logger = logging.getLogger('PAH_Optimized')
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    
    # Handler para archivo
    file_handler = logging.FileHandler(
        os.path.join(CARPETA_RESULTADOS, 'pah_optimized.log'),
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    
    # Handler para consola con formato optimizado
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # Formato optimizado para monitoreo
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)s | %(message)s',
        datefmt='%H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def safe_makedirs(path):
    """Crea directorios de forma segura"""
    if not path:
        raise ValueError("Intento de crear un directorio vac√≠o.")
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)
        # ===============================================
# üöÄ M√âTRICAS EEG OPTIMIZADAS (SIN RICCI)
# ===============================================

def kappa_topologico_optimizado(corr_matrix, threshold=0.5):
    """Curvatura topol√≥gica optimizada - REEMPLAZA curvatura de Ricci"""
    try:
        # Usar umbralizaci√≥n y medidas de centralidad m√°s eficientes
        bin_matrix = (np.abs(corr_matrix) > threshold).astype(np.float32)
        np.fill_diagonal(bin_matrix, 0)
        
        # Grado promedio normalizado (m√°s eficiente que Ricci)
        n_nodes = bin_matrix.shape[0]
        degrees = np.sum(bin_matrix, axis=0)
        avg_degree = np.mean(degrees)
        
        # Medida de curvatura basada en clustering local
        clustering_coeff = []
        for i in range(n_nodes):
            neighbors = np.where(bin_matrix[i] > 0)[0]
            if len(neighbors) < 2:
                clustering_coeff.append(0)
                continue
            
            # Conexiones entre vecinos
            subgraph = bin_matrix[np.ix_(neighbors, neighbors)]
            possible_edges = len(neighbors) * (len(neighbors) - 1)
            actual_edges = np.sum(subgraph)
            
            if possible_edges > 0:
                clustering_coeff.append(actual_edges / possible_edges)
            else:
                clustering_coeff.append(0)
        
        # Curvatura aproximada: combinaci√≥n de grado y clustering
        kappa = avg_degree * np.mean(clustering_coeff) * (n_nodes / 100.0)
        return max(0, kappa)  # Asegurar valor no negativo
        
    except Exception as e:
        return np.nan

def mutual_information_optimizado(x, y, bins=16):
    """Informaci√≥n mutua optimizada con manejo de memoria"""
    try:
        # Reducir bins si hay muchos datos para optimizar memoria
        if len(x) > 10000:
            bins = min(12, bins)
        
        # Normalizar datos para mejorar estabilidad
        x_norm = (x - np.mean(x)) / (np.std(x) + 1e-8)
        y_norm = (y - np.mean(y)) / (np.std(y) + 1e-8)
        
        c_xy = np.histogram2d(x_norm, y_norm, bins=bins)[0]
        return mutual_info_score(None, None, contingency=c_xy)
    except:
        return 0.0

def phi_h_optimizado(mi_matrix):
    """Integraci√≥n informacional optimizada"""
    try:
        # Usar solo triangular superior para eficiencia
        triu_indices = np.triu_indices_from(mi_matrix, k=1)
        mi_values = mi_matrix[triu_indices]
        valid_values = mi_values[~np.isnan(mi_values)]
        
        if len(valid_values) == 0:
            return 0.0
        
        return np.mean(valid_values)
    except:
        return 0.0

def lz_complexity_optimizado(binary_signal):
    """Complejidad LZ optimizada con l√≠mite de tama√±o"""
    try:
        # Limitar tama√±o para eficiencia
        if len(binary_signal) > 2000:
            binary_signal = binary_signal[::2]  # Submuestrear
        
        s = ''.join([str(int(i)) for i in binary_signal])
        i, c, l = 0, 1, 1
        n = len(s)
        
        while True:
            if i + l > n:
                break
            if s[i:i+l] not in s[0:i]:
                c += 1
                i += l
                l = 1
            else:
                l += 1
        return c
    except:
        return 1

def delta_pci_optimizado(seg1, seg2):
    """Delta PCI optimizado con submuestreo inteligente"""
    try:
        # Submuestrear si los segmentos son muy grandes
        if len(seg1) > 5000:
            step = len(seg1) // 2500
            seg1 = seg1[::step]
            seg2 = seg2[::step]
        
        med = np.median(np.concatenate([seg1, seg2]))
        bin1 = (seg1 > med).astype(int)
        bin2 = (seg2 > med).astype(int)
        
        lz1 = lz_complexity_optimizado(bin1)
        lz2 = lz_complexity_optimizado(bin2)
        
        return abs(lz1 - lz2)
    except:
        return 0
    # ===============================================
# üîß PROCESAMIENTO EDF OPTIMIZADO
# ===============================================

def buscar_archivos_edf_pares(carpeta_base):
    """Busca pares EDF con validaci√≥n optimizada"""
    pares_encontrados = []
    
    # Usar pathlib para b√∫squeda m√°s eficiente
    carpeta_path = Path(carpeta_base)
    archivos_psg = list(carpeta_path.glob("*-PSG.edf"))
    archivos_hypno = list(carpeta_path.glob("*-Hypnogram.edf"))
    
    print(f"üìÅ PSG encontrados: {len(archivos_psg)}")
    print(f"üìÅ Hypnogram encontrados: {len(archivos_hypno)}")
    
    # Crear mapeo eficiente
    hypno_map = {}
    for hypno_path in archivos_hypno:
        codigo_base = hypno_path.stem.replace("-Hypnogram", "")[:6]
        hypno_map[codigo_base] = hypno_path
    
    for psg_path in archivos_psg:
        codigo_base = psg_path.stem.replace("-PSG", "")[:6]
        
        if codigo_base in hypno_map:
            pares_encontrados.append((
                str(psg_path), 
                str(hypno_map[codigo_base]), 
                psg_path.stem.replace("-PSG", "")
            ))
    
    print(f"‚úÖ Pares EDF completos: {len(pares_encontrados)}")
    return pares_encontrados
def cargar_eeg_optimizado(psg_edf_path, dur_ventana_s=30):
    """Carga EEG con optimizaciones de memoria"""
    try:
        # Cargar con configuraci√≥n optimizada para memoria
        raw = mne.io.read_raw_edf(
            psg_edf_path, 
            preload=False,  # No cargar todo en memoria
            verbose=False
        )
        
        sfreq = raw.info['sfreq']
        total_samples = len(raw.times)
        ventana_muestras = int(dur_ventana_s * sfreq)
        n_ventanas = total_samples // ventana_muestras
        n_canales = len(raw.ch_names)
        
        print(f"üìä EEG: {n_ventanas} ventanas, {n_canales} canales, {sfreq}Hz")
        
        # Cargar por chunks para optimizar memoria
        eeg_data = []
        chunk_size = min(100, n_ventanas)  # Procesar en chunks
        
        for i in range(0, n_ventanas, chunk_size):
            end_i = min(i + chunk_size, n_ventanas)
            start_sample = i * ventana_muestras
            end_sample = end_i * ventana_muestras
            
            # Cargar chunk
            data_chunk = raw.get_data(start=start_sample, stop=end_sample)
            
            # Segmentar chunk
            for j in range(end_i - i):
                ventana_start = j * ventana_muestras
                ventana_end = (j + 1) * ventana_muestras
                if ventana_end <= data_chunk.shape[1]:
                    eeg_data.append(data_chunk[:, ventana_start:ventana_end])
            
            # Limpiar memoria
            del data_chunk
            gc.collect()
        
        print(f"‚úÖ EEG cargado: {len(eeg_data)} ventanas procesadas")
        return eeg_data
        
    except Exception as e:
        print(f"‚ùå Error cargando EEG {psg_edf_path}: {e}")
        raise

def calcular_metricas_batch_optimizado(df, eeg_data, batch_size=50):
    """Calcula m√©tricas en batches para optimizar memoria"""
    print(f"üßÆ Calculando m√©tricas PAH* para {len(df)} ventanas...")
    
    n_ventanas = min(len(df), len(eeg_data))
    
    # Inicializar columnas
    df['k_topo'] = np.nan
    df['phi_h'] = np.nan  
    df['delta_pci'] = np.nan
    
    # Procesar en batches
    for i in range(0, n_ventanas, batch_size):
        end_i = min(i + batch_size, n_ventanas)
        
        print(f"üìä Procesando ventanas {i:4d}-{end_i:4d} ({(end_i/n_ventanas)*100:.1f}%)")
        
        for j in range(i, end_i):
            try:
                datos_ventana = eeg_data[j]
                
                # Validar datos
                if np.any(np.isnan(datos_ventana)) or np.any(np.isinf(datos_ventana)):
                    continue
                
                # Calcular m√©tricas optimizadas
                if datos_ventana.shape[0] > 1:
                    # Correlaci√≥n con submuestreo si hay muchos canales
                    if datos_ventana.shape[0] > 20:
                        step = datos_ventana.shape[0] // 15
                        datos_sub = datos_ventana[::step]
                    else:
                        datos_sub = datos_ventana
                    
                    corr_matrix = np.corrcoef(datos_sub)
                    if not np.any(np.isnan(corr_matrix)):
                        df.at[j, 'k_topo'] = kappa_topologico_optimizado(corr_matrix)
                        
                        # Informaci√≥n mutua solo para subset de canales
                        n_ch = min(8, corr_matrix.shape[0])
                        mi_values = []
                        for ch1 in range(n_ch):
                            for ch2 in range(ch1+1, n_ch):
                                mi = mutual_information_optimizado(
                                    datos_sub[ch1], datos_sub[ch2]
                                )
                                mi_values.append(mi)
                        
                        df.at[j, 'phi_h'] = np.mean(mi_values) if mi_values else 0
                
                # Delta PCI
                if j > 0:
                    df.at[j, 'delta_pci'] = delta_pci_optimizado(
                        datos_ventana.flatten(), 
                        eeg_data[j-1].flatten()
                    )
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Error en ventana {j}: {e}")
                continue
        
        # Limpiar memoria cada batch
        gc.collect()
    
    # Estad√≠sticas de completitud
    completitud_k = (~df['k_topo'].isna()).sum() / len(df)
    completitud_phi = (~df['phi_h'].isna()).sum() / len(df)
    completitud_delta = (~df['delta_pci'].isna()).sum() / len(df)
    
    print(f"‚úÖ Completitud: Œ∫={completitud_k:.1%}, Œ¶={completitud_phi:.1%}, Œî={completitud_delta:.1%}")
    
    return df
# ===============================================
# üéØ DETECCI√ìN DE SINGULARIDADES OPTIMIZADA
# ===============================================

def cruza_horizonte_h_optimizado(k_topo, phi_h, delta_pci):
    """Detecci√≥n optimizada del Horizonte H* con umbrales calibrados"""
    return (
        not pd.isnull(k_topo) and k_topo >= K_TOPO_THRESHOLD and 
        not pd.isnull(phi_h) and phi_h >= PHI_H_THRESHOLD and 
        not pd.isnull(delta_pci) and delta_pci <= DELTA_PCI_THRESHOLD
    )

def buscar_singularidades_vectorizado(df, usar_filtros=True):
    """B√∫squeda vectorizada de singularidades para mayor eficiencia"""
    # Crear m√°scaras vectorizadas
    mask_k = (df['k_topo'] >= K_TOPO_THRESHOLD) & df['k_topo'].notna()
    mask_phi = (df['phi_h'] >= PHI_H_THRESHOLD) & df['phi_h'].notna()
    mask_delta = (np.abs(df['delta_pci']) <= DELTA_PCI_THRESHOLD) & df['delta_pci'].notna()
    
    # Combinar m√°scaras
    mask_horizonte = mask_k & mask_phi & mask_delta
    indices_candidatos = df.index[mask_horizonte].tolist()
    
    print(f"üéØ Candidatos Horizonte H*: {len(indices_candidatos)} ({len(indices_candidatos)/len(df)*100:.3f}%)")
    
    if not usar_filtros or len(indices_candidatos) == 0:
        return indices_candidatos
    
    # Filtro de intensidad optimizado
    try:
        df_candidatos = df.loc[indices_candidatos]
        intensidades = (df_candidatos['k_topo'] + 
                       df_candidatos['phi_h'] + 
                       df_candidatos['delta_pci']).values
        
        umbral_intensidad = np.percentile(intensidades, 50)
        mask_intensidad = intensidades >= umbral_intensidad
        
        indices_filtrados = np.array(indices_candidatos)[mask_intensidad].tolist()
    except:
        indices_filtrados = indices_candidatos
    
    # Clustering temporal optimizado
    if len(indices_filtrados) > 1:
        indices_finales = aplicar_clustering_temporal_optimizado(indices_filtrados)
    else:
        indices_finales = indices_filtrados
    
    print(f"üéØ Singularidades finales: {len(indices_finales)} ({len(indices_finales)/len(df)*100:.3f}%)")
    
    return indices_finales
def aplicar_clustering_temporal_optimizado(indices, ventana_clustering=2):
    """Clustering temporal vectorizado"""
    if len(indices) <= 1:
        return indices
    
    indices_arr = np.array(sorted(indices))
    diffs = np.diff(indices_arr)
    
    # Encontrar clusters donde la diferencia es <= ventana_clustering
    cluster_breaks = np.where(diffs > ventana_clustering)[0] + 1
    cluster_starts = np.concatenate([[0], cluster_breaks])
    cluster_ends = np.concatenate([cluster_breaks, [len(indices_arr)]])
    
    indices_representativos = []
    for start, end in zip(cluster_starts, cluster_ends):
        cluster = indices_arr[start:end]
        # Seleccionar el del centro del cluster
        centro = len(cluster) // 2
        indices_representativos.append(cluster[centro])
    
    return indices_representativos.tolist()

def detectar_transiciones_vectorizado(df):
    """Detecci√≥n vectorizada de transiciones"""
    if "estado" not in df.columns:
        print("‚ö†Ô∏è  No hay columna 'estado' para detectar transiciones")
        return [], []
    
    estados = df["estado"].fillna(-1).values
    
    # Vectorizar detecci√≥n de cambios
    cambios = np.diff(estados) != 0
    indices_cambio = np.where(cambios)[0] + 1
    
    despertares = []
    dormidas = []
    
    for i in indices_cambio:
        if i >= len(estados):
            continue
            
        estado_prev = estados[i-1]
        estado_actual = estados[i]
        
        # Despertar: cualquier estado ‚Üí vigilia (0)
        if estado_prev != 0 and estado_actual == 0:
            despertares.append(i)
        # Dormida: vigilia (0) ‚Üí cualquier estado de sue√±o
        elif estado_prev == 0 and estado_actual != 0 and estado_actual != -1:
            dormidas.append(i)
    
    print(f"üîÑ Transiciones: {len(despertares)} despertares, {len(dormidas)} dormidas")
    return despertares, dormidas
# ===============================================
# üöÄ PIPELINE PRINCIPAL OPTIMIZADO
# ===============================================

def clasificar_singularidades_optimizada(singularidad_idx, despertares, dormidas):
    """Clasificaci√≥n optimizada de singularidades"""
    if not singularidad_idx:
        return {}
    
    # Crear arrays para vectorizaci√≥n
    sing_arr = np.array(singularidad_idx)
    trans_arr = np.array(despertares + dormidas)
    trans_types = ['convergente'] * len(despertares) + ['divergente'] * len(dormidas)
    
    singularidad_info = {}
    
    for i, idx in enumerate(sing_arr):
        # Encontrar transici√≥n m√°s cercana
        distancias = np.abs(trans_arr - idx)
        min_idx = np.argmin(distancias)
        distancia = distancias[min_idx]
        tipo_trans = trans_types[min_idx]
        
        if distancia <= VENTANA_MARGIN:
            etiqueta = tipo_trans
            confianza = 1.0 - (distancia / VENTANA_MARGIN) * 0.3
        elif distancia <= VENTANA_MARGIN_LEVE:
            etiqueta = f"{tipo_trans}_leve"
            confianza = 0.6 - ((distancia - VENTANA_MARGIN) / 
                             (VENTANA_MARGIN_LEVE - VENTANA_MARGIN)) * 0.4
        else:
            etiqueta = "sin_clasificar"
            confianza = 0.0
        
        singularidad_info[idx] = {
            'etiqueta': etiqueta,
            'desfase': idx - trans_arr[min_idx],
            'confianza': confianza,
            'transicion_cercana': trans_arr[min_idx]
        }
    
    return singularidad_info

def procesar_archivo_con_checkpoint(psg_path, hypnogram_path, base_name, 
                                   carpeta_resultados, progress_manager):
    """Procesa un archivo con checkpoint autom√°tico"""
    start_time = time.time()
    
    try:
        print(f"\nüéØ Procesando: {base_name}")
        print(f"üìä Progreso: {progress_manager.get_progress_percentage():.1f}% | ETA: {progress_manager.get_eta()}")
        
        # 1. Cargar EEG optimizado
        print("üì• Cargando datos EEG...")
        eeg_data = cargar_eeg_optimizado(psg_path, TAMANO_VENTANA)
        
        # 2. Procesar hypnogram
        print("üìã Procesando hypnogram...")
        df_hypnogram = procesar_hypnogram_edf_directo(hypnogram_path)
        
        if df_hypnogram is None:
            print(f"‚ùå Error procesando hypnogram: {hypnogram_path}")
            return None
        
        # 3. Alinear datos
        n_ventanas = min(len(eeg_data), len(df_hypnogram))
        eeg_data_alineado = eeg_data[:n_ventanas]
        df_alineado = df_hypnogram.iloc[:n_ventanas].copy()
        
        print(f"üîó Alineaci√≥n: {n_ventanas} ventanas")
        
        # 4. Calcular m√©tricas PAH* optimizadas
        print("üßÆ Calculando m√©tricas PAH* optimizadas...")
        df_con_metricas = calcular_metricas_batch_optimizado(df_alineado, eeg_data_alineado)
        
        # 5. Detectar transiciones vectorizado
        despertares, dormidas = detectar_transiciones_vectorizado(df_con_metricas)
        
        if len(despertares) == 0:
            print(f"‚ö†Ô∏è  Sin despertares en {base_name}")
            progress_manager.mark_file_processed(base_name, False)
            return None
        
        # 6. Buscar singularidades optimizado
        print("üîç Buscando singularidades Horizonte H*...")
        singularidad_idx = buscar_singularidades_vectorizado(df_con_metricas)
        
        # 7. Clasificar singularidades
        singularidad_info = clasificar_singularidades_optimizada(
            singularidad_idx, despertares, dormidas
        )
        
        # 8. Calcular m√©tricas de calidad
        metricas_calidad = calcular_metricas_calidad_rapidas(
            singularidad_idx, despertares + dormidas, df_con_metricas
        )
        
        # 9. Guardar resultados con estructura optimizada
        carpeta_sujeto = os.path.join(carpeta_resultados, f"sujeto_{base_name}")
        safe_makedirs(carpeta_sujeto)
        
        guardar_resultados_optimizado(
            df_con_metricas, singularidad_idx, singularidad_info,
            despertares, dormidas, metricas_calidad,
            carpeta_sujeto, base_name
        )
        
        # 10. Actualizar progreso
        processing_time = time.time() - start_time
        progress_manager.update_stats(
            len(singularidad_idx), 
            len(despertares) + len(dormidas),
            processing_time
        )
        progress_manager.mark_file_processed(base_name, True)
        
        # 11. Limpiar memoria
        del eeg_data, eeg_data_alineado, df_con_metricas
        gc.collect()
        
        print(f"‚úÖ Completado en {processing_time:.1f}s")
        print(f"   Singularidades: {len(singularidad_idx)}")
        print(f"   F1-Score: {metricas_calidad.get('f1_score', 0):.3f}")
        
        return generar_resumen_optimizado(
            base_name, despertares, dormidas, singularidad_idx,
            singularidad_info, metricas_calidad
        )
        
    except Exception as e:
        print(f"‚ùå Error procesando {base_name}: {e}")
        progress_manager.mark_file_processed(base_name, False)
        return None

def calcular_metricas_calidad_rapidas(singularidad_idx, transiciones, df, margen=3):
    """M√©tricas de calidad optimizadas"""
    if not singularidad_idx or not transiciones:
        return {
            'cobertura_transiciones': 0, 'especificidad_temporal': 0, 'precision': 0,
            'recall': 0, 'f1_score': 0, 'eficiencia_deteccion': 0
        }
    
    # Vectorizar c√°lculos
    sing_arr = np.array(singularidad_idx)
    trans_arr = np.array(transiciones)
    
    # Cobertura de transiciones
    transiciones_cubiertas = 0
    for t in trans_arr:
        if np.any(np.abs(sing_arr - t) <= margen):
            transiciones_cubiertas += 1
    
    cobertura_transiciones = transiciones_cubiertas / len(trans_arr)
    
    # Especificidad temporal
    singularidades_especificas = 0
    for s in sing_arr:
        if np.any(np.abs(trans_arr - s) <= margen):
            singularidades_especificas += 1
    
    especificidad_temporal = singularidades_especificas / len(sing_arr)
    # M√©tricas de clasificaci√≥n
    verdaderos_positivos = singularidades_especificas
    falsos_positivos = len(sing_arr) - singularidades_especificas
    falsos_negativos = len(trans_arr) - transiciones_cubiertas
    
    precision = (verdaderos_positivos / (verdaderos_positivos + falsos_positivos) 
                if (verdaderos_positivos + falsos_positivos) > 0 else 0)
    recall = (verdaderos_positivos / (verdaderos_positivos + falsos_negativos) 
             if (verdaderos_positivos + falsos_negativos) > 0 else 0)
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    tasa_singularidades = len(sing_arr) / len(df) if len(df) > 0 else 0
    eficiencia_deteccion = cobertura_transiciones / (tasa_singularidades + 1e-6)
    
    return {
        'cobertura_transiciones': cobertura_transiciones,
        'especificidad_temporal': especificidad_temporal,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'eficiencia_deteccion': eficiencia_deteccion
    }

def procesar_hypnogram_edf_directo(hypnogram_path):
    """Procesa hypnogram con debug extendido"""
    print(f"üîç DEBUG: Procesando {os.path.basename(hypnogram_path)}")
    
    try:
        # M√©todo 1: Intentar con MNE
        try:
            raw = mne.io.read_raw_edf(hypnogram_path, preload=True, verbose=False)
            print(f"‚úÖ MNE carg√≥ el archivo")
            
            annotations = raw.annotations
            print(f"üìã Annotations encontradas: {len(annotations)}")
            
            if len(annotations) > 0:
                df_result = procesar_anotaciones_mne(annotations)
                if df_result is not None and len(df_result) > 0:
                    print(f"‚úÖ MNE proces√≥: {len(df_result)} ventanas")
                    return df_result
                else:
                    print("‚ö†Ô∏è  MNE annotations vac√≠as, probando parser manual")
            else:
                print("‚ö†Ô∏è  Sin annotations en MNE, probando parser manual")
        except Exception as e:
            print(f"‚ö†Ô∏è  MNE fall√≥: {e}")
        
        # M√©todo 2: Parser manual
        print("üîç Intentando parser manual...")
        df_manual = parse_hypnogram_edf(hypnogram_path)
        
        if df_manual is not None and len(df_manual) > 0:
            print(f"‚úÖ Parser manual exitoso: {len(df_manual)} ventanas")
            return df_manual
        else:
            print("‚ùå Parser manual tambi√©n fall√≥")
            
        # M√©todo 3: Inspecci√≥n directa del archivo
        print("üîç Inspeccionando archivo directamente...")
        with open(hypnogram_path, "rb") as f:
            primeros_bytes = f.read(1000)
            print(f"üìã Primeros bytes: {primeros_bytes[:100]}")
        
        return None
        
    except Exception as e:
        print(f"‚ùå Error cr√≠tico procesando hypnogram: {e}")
        return None

def parse_hypnogram_edf(file_path):
    """Parser optimizado de hypnogram"""
    SLEEP_STAGE_MAP = {"W": 0, "1": 1, "2": 2, "3": 3, "4": 4, "R": 5, "?": -1}
    
    try:
        with open(file_path, "r", encoding="latin1") as f:
            txt = f.read()
        
        pattern = re.compile(r"\+(\d+)\x15(\d+)\x14Sleep stage (\w)\x14")
        matches = pattern.findall(txt)
        
        rows = []
        ventana = 0
        
        for ini, dur, stage in matches:
            estado = SLEEP_STAGE_MAP.get(stage, -1)
            n_ventanas = int(dur) // TAMANO_VENTANA
            
            for i in range(n_ventanas):
                t_inicio = int(ini) + i * TAMANO_VENTANA
                t_centro = t_inicio + TAMANO_VENTANA // 2
                
                rows.append({
                    "ventana": ventana,
                    "t_inicio_s": t_inicio,
                    "k_topo": None,
                    "phi_h": None,
                    "delta_pci": None,
                    "estado": estado,
                    "t_centro_s": t_centro,
                })
                ventana += 1
        
        return pd.DataFrame(rows)
    except Exception as e:
        print(f"‚ùå Error parseando hypnogram: {e}")
        return None

def procesar_anotaciones_mne(annotations):
    """Procesa anotaciones MNE con debug extendido"""
    SLEEP_STAGE_MAP = {
        "Sleep stage W": 0, "Sleep stage 1": 1, "Sleep stage 2": 2,
        "Sleep stage 3": 3, "Sleep stage 4": 4, "Sleep stage R": 5,
        "W": 0, "1": 1, "2": 2, "3": 3, "4": 4, "R": 5, "?": -1
    }
    
    print(f"üîç DEBUG Annotations:")
    print(f"   Total: {len(annotations)}")
    
    # Mostrar las primeras 5 annotations para debug
    for i in range(min(5, len(annotations))):
        print(f"   [{i}] Onset: {annotations.onset[i]:.1f}s, "
              f"Duration: {annotations.duration[i]:.1f}s, "
              f"Description: '{annotations.description[i]}'")
    
    rows = []
    ventana = 0
    
    for i, (onset, duration, description) in enumerate(zip(
        annotations.onset, annotations.duration, annotations.description)):
        
        estado = SLEEP_STAGE_MAP.get(description, -1)
        if estado == -1:
            # Intentar mapeo alternativo
            desc_clean = description.strip().upper()
            if 'W' in desc_clean:
                estado = 0
            elif 'REM' in desc_clean or 'R' in desc_clean:
                estado = 5
            elif '1' in desc_clean:
                estado = 1
            elif '2' in desc_clean:
                estado = 2
            elif '3' in desc_clean:
                estado = 3
            elif '4' in desc_clean:
                estado = 4
            else:
                print(f"‚ö†Ô∏è  Estado no reconocido: '{description}' -> usando -1")
        
        n_ventanas = int(duration) // TAMANO_VENTANA
        
        for j in range(n_ventanas):
            t_inicio = int(onset) + j * TAMANO_VENTANA
            t_centro = t_inicio + TAMANO_VENTANA // 2
            
            rows.append({
                "ventana": ventana,
                "t_inicio_s": t_inicio,
                "k_topo": None,
                "phi_h": None,
                "delta_pci": None,
                "estado": estado,
                "t_centro_s": t_centro,
            })
            ventana += 1
    
    if rows:
        df = pd.DataFrame(rows)
        print(f"‚úÖ DataFrame creado: {len(df)} filas")
        print(f"   Estados √∫nicos: {sorted(df['estado'].unique())}")
        return df
    else:
        print("‚ùå No se pudieron crear filas del DataFrame")
        return None
def guardar_resultados_optimizado(df, singularidad_idx, singularidad_info, despertares, 
                                 dormidas, metricas_calidad, carpeta, base_name):
    """Guarda resultados con estructura optimizada"""
    
    # 1. CSV principal con flags
    df_resultado = df.copy()
    df_resultado['es_singularidad'] = df_resultado.index.isin(singularidad_idx)
    df_resultado['es_despertar'] = df_resultado.index.isin(despertares)
    df_resultado['es_dormida'] = df_resultado.index.isin(dormidas)
    
    # Agregar info de singularidades
    df_resultado['tipo_singularidad'] = 'sin_clasificar'
    df_resultado['confianza'] = 0.0
    df_resultado['cruza_horizonte_h'] = False
    
    for idx, info in singularidad_info.items():
        if idx < len(df_resultado):
            df_resultado.at[idx, 'tipo_singularidad'] = info.get('etiqueta', 'sin_clasificar')
            df_resultado.at[idx, 'confianza'] = info.get('confianza', 0.0)
            df_resultado.at[idx, 'cruza_horizonte_h'] = True
    
    # Guardar CSV principal
    csv_path = os.path.join(carpeta, f"{base_name}_PAH_resultados.csv")
    df_resultado.to_csv(csv_path, index=False)
    # 2. JSON con resumen y m√©tricas
    resumen = {
        'modelo': 'PAH* V2.1 - Optimizado i7-11800H',
        'fecha': time.strftime('%Y-%m-%d %H:%M:%S'),
        'archivo': base_name,
        'hardware': 'i7-11800H, 32GB RAM, RTX 3050 Ti',
        'umbrales': {
            'k_topo': float(K_TOPO_THRESHOLD),
            'phi_h': float(PHI_H_THRESHOLD),
            'delta_pci': float(DELTA_PCI_THRESHOLD)
        },
        'resultados': {
            'n_ventanas': len(df),
            'n_singularidades': len(singularidad_idx),
            'n_despertares': len(despertares),
            'n_dormidas': len(dormidas),
            'tasa_deteccion': len(singularidad_idx) / len(df) * 100
        },
        'metricas_calidad': metricas_calidad,
        'clasificacion_singularidades': {
            tipo: sum(1 for info in singularidad_info.values() 
                     if info.get('etiqueta') == tipo)
            for tipo in ['convergente', 'divergente', 'convergente_leve', 'divergente_leve', 'sin_clasificar']
        }
    }
    
    json_path = os.path.join(carpeta, f"{base_name}_resumen.json")
    with open(json_path, 'w') as f:
        json.dump(resumen, f, indent=2, default=str)

def generar_resumen_optimizado(base_name, despertares, dormidas, singularidad_idx,
                              singularidad_info, metricas_calidad):
    """Genera resumen optimizado"""
    conteo_tipos = {}
    for info in singularidad_info.values():
        tipo = info.get('etiqueta', 'sin_clasificar')
        conteo_tipos[tipo] = conteo_tipos.get(tipo, 0) + 1
    
    return {
        "archivo": base_name,
        "version": "V2.1_OPTIMIZADO_i7",
        "n_despertares": len(despertares),
        "n_dormidas": len(dormidas),
        "n_singularidades": len(singularidad_idx),
        "tipos_singularidades": conteo_tipos,
        "metricas": metricas_calidad
    }
# ===============================================
# üöÄ PIPELINE PRINCIPAL CON CHECKPOINTS
# ===============================================

def main_optimizado():
    """Pipeline principal optimizado para i7-11800H"""
    
    # Banner de inicio
    print("=" * 80)
    print("üöÄ MODELO PAH* V2.1 - OPTIMIZADO PARA i7-11800H")
    print("   Desarrollado por: Camilo Alejandro Sj√∂berg Tala")
    print("=" * 80)
    print(f"üíª Hardware: i7-11800H ({CPU_CORES}C/{CPU_THREADS}T) | 32GB RAM | RTX 3050 Ti")
    print(f"‚öôÔ∏è  Workers: {OPTIMAL_WORKERS} | Chunk Size: {CHUNK_SIZE}")
    print(f"üéØ Umbrales: Œ∫‚â•{K_TOPO_THRESHOLD:.3f} | Œ¶‚â•{PHI_H_THRESHOLD:.3f} | Œî‚â§{DELTA_PCI_THRESHOLD:.3f}")
    print("=" * 80)
    
    # Configurar directorios
    safe_makedirs(CARPETA_RESULTADOS)
    safe_makedirs(CHECKPOINT_DIR)
    
    # Configurar logging optimizado
    logger = setup_optimized_logging()
    logger.info("üöÄ Iniciando Modelo PAH* V2.1 Optimizado")
    
    # Inicializar gestor de progreso
    progress_manager = ProgressManager(PROGRESS_FILE)
    
    # Buscar archivos EDF
    print("\nüìÅ Buscando archivos EDF...")
    pares_edf = buscar_archivos_edf_pares(CARPETA_BASE)
    
    if not pares_edf:
        print("‚ùå No se encontraron pares EDF")
        return
    
    # Actualizar total de archivos
    progress_manager.data['total_files'] = len(pares_edf)
    progress_manager.save_progress()
    
    print(f"‚úÖ Encontrados {len(pares_edf)} pares EDF")
    # Filtrar archivos ya procesados
    pendientes = []
    for psg_path, hypno_path, base_name in pares_edf:
        if not progress_manager.is_file_processed(base_name):
            pendientes.append((psg_path, hypno_path, base_name))
        else:
            print(f"‚è≠Ô∏è  Saltando {base_name} (ya procesado)")
    
    if not pendientes:
        print("‚úÖ Todos los archivos ya fueron procesados")
        generar_reporte_final_optimizado(progress_manager)
        return
    
    print(f"\nüîÑ Archivos pendientes: {len(pendientes)}")
    print(f"üìä Progreso actual: {progress_manager.get_progress_percentage():.1f}%")
    
    # Procesar archivos con checkpoints
    print("\nüöÄ Iniciando procesamiento con checkpoints...")
    resultados_batch = []
    
    for i, (psg_path, hypno_path, base_name) in enumerate(pendientes):
        print(f"\n{'='*60}")
        print(f"üìã Archivo {i+1}/{len(pendientes)}: {base_name}")
        print(f"üìä Progreso global: {progress_manager.get_progress_percentage():.1f}%")
        print(f"‚è∞ ETA: {progress_manager.get_eta()}")
        print(f"üíæ RAM libre: {psutil.virtual_memory().available / (1024**3):.1f}GB")
        
        try:
            resultado = procesar_archivo_con_checkpoint(
                psg_path, hypno_path, base_name, 
                CARPETA_RESULTADOS, progress_manager
            )
            
            if resultado:
                resultados_batch.append(resultado)
                print(f"‚úÖ {base_name} procesado exitosamente")
            else:
                print(f"‚ö†Ô∏è  {base_name} sin resultados v√°lidos")
        
        except KeyboardInterrupt:
            print(f"\nüõë Interrupci√≥n del usuario en {base_name}")
            print(f"üíæ Progreso guardado. Usa el mismo comando para continuar.")
            break
        except Exception as e:
            print(f"‚ùå Error cr√≠tico en {base_name}: {e}")
            continue
    
    # Generar reporte final
    print(f"\nüéâ Procesamiento completado!")
    print(f"üìä Archivos procesados: {len(progress_manager.data['processed_files'])}")
    print(f"üìä Archivos fallidos: {len(progress_manager.data['failed_files'])}")
    
    generar_reporte_final_optimizado(progress_manager)
    
    print(f"\nüíæ Resultados guardados en: {CARPETA_RESULTADOS}")
    print("üî¨ Listos para an√°lisis cient√≠fico del Modelo PAH*")
    def generar_reporte_final_optimizado(progress_manager):
        """Genera reporte final con estad√≠sticas del hardware"""
    
        reporte_path = os.path.join(CARPETA_RESULTADOS, "reporte_final_optimizado.txt")
        
        with open(reporte_path, "w", encoding='utf-8') as f:
            f.write("=" * 100 + "\n")
            f.write("üöÄ REPORTE FINAL - MODELO PAH* V2.1 OPTIMIZADO\n")
            f.write("   Optimizado para: Intel i7-11800H + 32GB RAM + RTX 3050 Ti\n")
            f.write("   Desarrollado por: Camilo Alejandro Sj√∂berg Tala\n")
            f.write("=" * 100 + "\n\n")
            
            f.write(f"üìÖ Fecha: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"üíª Hardware: i7-11800H ({CPU_CORES}C/{CPU_THREADS}T), 32GB RAM, RTX 3050 Ti\n")
            f.write(f"‚öôÔ∏è  Configuraci√≥n: {OPTIMAL_WORKERS} workers, chunks de {CHUNK_SIZE}\n\n")
            
            # Estad√≠sticas de procesamiento
            stats = progress_manager.data['stats']
            total_files = progress_manager.data['total_files']
            processed_files = len(progress_manager.data['processed_files'])
            failed_files = len(progress_manager.data['failed_files'])
            
            f.write("üìä ESTAD√çSTICAS DE PROCESAMIENTO\n")
            f.write("-" * 50 + "\n")
            f.write(f"Total de archivos: {total_files}\n")
            f.write(f"Archivos procesados: {processed_files} ({processed_files/total_files*100:.1f}%)\n")
            f.write(f"Archivos fallidos: {failed_files} ({failed_files/total_files*100:.1f}%)\n")
            f.write(f"Tiempo total: {stats['processing_time']:.1f}s ({stats['processing_time']/60:.1f}min)\n")
            f.write(f"Tiempo promedio por archivo: {stats['processing_time']/max(1,processed_files):.1f}s\n\n")
            # Resultados del Modelo PAH*
            f.write("üéØ RESULTADOS DEL MODELO PAH* V2.1\n")
            f.write("-" * 50 + "\n")
            f.write(f"Total singularidades detectadas: {stats['total_singularities']:,}\n")
            f.write(f"Total transiciones analizadas: {stats['total_transitions']:,}\n")
            
            if processed_files > 0:
                ventanas_estimadas = processed_files * 2800  # Estimaci√≥n
                tasa_deteccion = stats['total_singularities'] / ventanas_estimadas * 100
                f.write(f"Tasa de detecci√≥n estimada: {tasa_deteccion:.3f}%\n")
                
                # Evaluar √©xito
                exito = 1 <= tasa_deteccion <= 8
                f.write(f"‚úÖ Objetivo cumplido: {'S√ç' if exito else 'PARCIAL'}\n")
            
            f.write("\nüöÄ OPTIMIZACIONES IMPLEMENTADAS\n")
            f.write("-" * 50 + "\n")
            f.write("‚Ä¢ Eliminaci√≥n de curvatura de Ricci (muy costosa computacionalmente)\n")
            f.write("‚Ä¢ Curvatura topol√≥gica optimizada con clustering local\n")
            f.write("‚Ä¢ Procesamiento vectorizado de transiciones y singularidades\n")
            f.write("‚Ä¢ Carga de EEG por chunks para optimizar memoria\n")
            f.write("‚Ä¢ Sistema de checkpoints para recuperaci√≥n autom√°tica\n")
            f.write("‚Ä¢ Configuraci√≥n espec√≠fica para i7-11800H (12 workers)\n")
            f.write("‚Ä¢ Garbage collection autom√°tico entre archivos\n")
            f.write("‚Ä¢ Progreso persistente con ETA en tiempo real\n\n")
            f.write("üéØ CONCLUSI√ìN\n")
            f.write("-" * 50 + "\n")
            f.write("El Modelo PAH* V2.1 ha sido optimizado exitosamente\n")
            f.write("para hardware espec√≠fico i7-11800H, logrando:\n\n")
            f.write("‚úÖ Procesamiento eficiente sin saturar recursos\n")
            f.write("‚úÖ Recuperaci√≥n autom√°tica ante interrupciones\n")
            f.write("‚úÖ Monitoreo en tiempo real del progreso\n")
            f.write("‚úÖ Detecci√≥n del Horizonte H* con umbrales calibrados\n\n")
            f.write("Este representa un avance significativo en la\n")
            f.write("operacionalizaci√≥n del Modelo PAH* para investigaci√≥n\n")
            f.write("neurocient√≠fica de alto rendimiento.\n\n")
            f.write(f"Camilo Alejandro Sj√∂berg Tala\n")
            f.write(f"Investigador Independiente - Modelo PAH*\n")
    
    reporte_path = os.path.join(CARPETA_RESULTADOS, "reporte_final_optimizado.txt")
    
    with open(reporte_path, "w", encoding='utf-8') as f:
        f.write("=" * 100 + "\n")
        f.write("üöÄ REPORTE FINAL - MODELO PAH* V2.1 OPTIMIZADO\n")
        f.write("   Optimizado para: Intel i7-11800H + 32GB RAM + RTX 3050 Ti\n")
        f.write("   Desarrollado por: Camilo Alejandro Sj√∂berg Tala\n")
        f.write("=" * 100 + "\n\n")
        
        f.write(f"üìÖ Fecha: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"üíª Hardware: i7-11800H ({CPU_CORES}C/{CPU_THREADS}T), 32GB RAM, RTX 3050 Ti\n")
        f.write(f"‚öôÔ∏è  Configuraci√≥n: {OPTIMAL_WORKERS} workers, chunks de {CHUNK_SIZE}\n\n")
        
        # Estad√≠sticas de procesamiento
        stats = progress_manager.data['stats']
        total_files = progress_manager.data['total_files']
        processed_files = len(progress_manager.data['processed_files'])
        failed_files = len(progress_manager.data['failed_files'])
        
        f.write("üìä ESTAD√çSTICAS DE PROCESAMIENTO\n")
        f.write("-" * 50 + "\n")
        f.write(f"Total de archivos: {total_files}\n")
        f.write(f"Archivos procesados: {processed_files} ({processed_files/total_files*100:.1f}%)\n")
        f.write(f"Archivos fallidos: {failed_files} ({failed_files/total_files*100:.1f}%)\n")
        f.write(f"Tiempo total: {stats['processing_time']:.1f}s ({stats['processing_time']/60:.1f}min)\n")
        f.write(f"Tiempo promedio por archivo: {stats['processing_time']/max(1,processed_files):.1f}s\n\n")
        # Resultados del Modelo PAH*
        f.write("üéØ RESULTADOS DEL MODELO PAH* V2.1\n")
        f.write("-" * 50 + "\n")
        f.write(f"Total singularidades detectadas: {stats['total_singularities']:,}\n")
        f.write(f"Total transiciones analizadas: {stats['total_transitions']:,}\n")
        
        if processed_files > 0:
            ventanas_estimadas = processed_files * 2800  # Estimaci√≥n
            tasa_deteccion = stats['total_singularities'] / ventanas_estimadas * 100
            f.write(f"Tasa de detecci√≥n estimada: {tasa_deteccion:.3f}%\n")
            
            # Evaluar √©xito
            exito = 1 <= tasa_deteccion <= 8
            f.write(f"‚úÖ Objetivo cumplido: {'S√ç' if exito else 'PARCIAL'}\n")
        
        f.write("\nüöÄ OPTIMIZACIONES IMPLEMENTADAS\n")
        f.write("-" * 50 + "\n")
        f.write("‚Ä¢ Eliminaci√≥n de curvatura de Ricci (muy costosa computacionalmente)\n")
        f.write("‚Ä¢ Curvatura topol√≥gica optimizada con clustering local\n")
        f.write("‚Ä¢ Procesamiento vectorizado de transiciones y singularidades\n")
        f.write("‚Ä¢ Carga de EEG por chunks para optimizar memoria\n")
        f.write("‚Ä¢ Sistema de checkpoints para recuperaci√≥n autom√°tica\n")
        f.write("‚Ä¢ Configuraci√≥n espec√≠fica para i7-11800H (12 workers)\n")
        f.write("‚Ä¢ Garbage collection autom√°tico entre archivos\n")
        f.write("‚Ä¢ Progreso persistente con ETA en tiempo real\n\n")
        f.write("üéØ CONCLUSI√ìN\n")
        f.write("-" * 50 + "\n")
        f.write("El Modelo PAH* V2.1 ha sido optimizado exitosamente\n")
        f.write("para hardware espec√≠fico i7-11800H, logrando:\n\n")
        f.write("‚úÖ Procesamiento eficiente sin saturar recursos\n")
        f.write("‚úÖ Recuperaci√≥n autom√°tica ante interrupciones\n")
        f.write("‚úÖ Monitoreo en tiempo real del progreso\n")
        f.write("‚úÖ Detecci√≥n del Horizonte H* con umbrales calibrados\n\n")
        f.write("Este representa un avance significativo en la\n")
        f.write("operacionalizaci√≥n del Modelo PAH* para investigaci√≥n\n")
        f.write("neurocient√≠fica de alto rendimiento.\n\n")
        f.write(f"Camilo Alejandro Sj√∂berg Tala\n")
        f.write(f"Investigador Independiente - Modelo PAH*\n")

def mostrar_estadisticas_sistema():
    """Muestra estad√≠sticas del sistema para verificar optimizaci√≥n"""
    print("\nüîç DIAGN√ìSTICO DEL SISTEMA")
    print("-" * 40)
    print(f"üíª CPU: {psutil.cpu_count(logical=False)}C/{psutil.cpu_count()}T")
    print(f"üß† RAM: {psutil.virtual_memory().total / (1024**3):.1f}GB total")
    print(f"üíæ RAM libre: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    print(f"üìä Uso CPU: {psutil.cpu_percent(interval=1):.1f}%")
    print(f"‚öôÔ∏è  Workers configurados: {OPTIMAL_WORKERS}")
    print(f"üéØ Configuraci√≥n √≥ptima: {'‚úÖ S√ç' if OPTIMAL_WORKERS <= CPU_THREADS else '‚ùå NO'}")
    # ===============================================
# üöÄ PUNTO DE ENTRADA PRINCIPAL
# ===============================================

if __name__ == "__main__":
    try:
        # Mostrar diagn√≥stico del sistema
        mostrar_estadisticas_sistema()
        
        # Ejecutar pipeline optimizado
        main_optimizado()
        
    except KeyboardInterrupt:
        print(f"\nüõë Proceso interrumpido por el usuario")
        print(f"üíæ El progreso ha sido guardado autom√°ticamente")
        print(f"üîÑ Ejecuta el mismo comando para continuar donde qued√≥")
    except Exception as e:
        print(f"\n‚ùå Error cr√≠tico: {e}")
        print(f"üíæ Revisa los logs en: {CARPETA_RESULTADOS}")
        # ===============================================
# üîß FUNCIONES ADICIONALES DE DIAGN√ìSTICO
# ===============================================

def verificar_dependencias():
    """Verifica que todas las dependencias est√©n instaladas"""
    dependencias_requeridas = [
        'numpy', 'pandas', 'matplotlib', 'sklearn', 'mne', 'tqdm', 
        'networkx', 'scipy', 'psutil', 'pathlib'
    ]
    
    print("üîç Verificando dependencias...")
    dependencias_faltantes = []
    
    for dep in dependencias_requeridas:
        try:
            __import__(dep)
            print(f"‚úÖ {dep}")
        except ImportError:
            dependencias_faltantes.append(dep)
            print(f"‚ùå {dep} - FALTANTE")
    
    if dependencias_faltantes:
        print(f"\n‚ùå Dependencias faltantes: {dependencias_faltantes}")
        print("üí° Instala con: pip install " + " ".join(dependencias_faltantes))
        return False
    else:
        print("‚úÖ Todas las dependencias est√°n instaladas")
        return True

def test_modelo_pah():
    """Test b√°sico del modelo PAH* con datos sint√©ticos"""
    print("\nüß™ Ejecutando test b√°sico del Modelo PAH*...")
    
    # Generar datos sint√©ticos
    n_ventanas = 100
    n_canales = 8
    
    # Crear DataFrame de prueba
    df_test = pd.DataFrame({
        'ventana': range(n_ventanas),
        'k_topo': np.random.uniform(0, 5, n_ventanas),
        'phi_h': np.random.uniform(0, 1, n_ventanas),
        'delta_pci': np.random.uniform(0, 100, n_ventanas),
        'estado': np.random.choice([0, 1, 2, 3, 5], n_ventanas)
    })
    
    # Test de detecci√≥n de singularidades
    print("üîç Testing b√∫squeda de singularidades...")
    singularidades = buscar_singularidades_vectorizado(df_test)
    print(f"‚úÖ Singularidades encontradas: {len(singularidades)}")
    
    # Test de detecci√≥n de transiciones
    print("üîç Testing detecci√≥n de transiciones...")
    despertares, dormidas = detectar_transiciones_vectorizado(df_test)
    print(f"‚úÖ Transiciones: {len(despertares)} despertares, {len(dormidas)} dormidas")
    
    # Test de clasificaci√≥n
    print("üîç Testing clasificaci√≥n...")
    clasificacion = clasificar_singularidades_optimizada(singularidades, despertares, dormidas)
    print(f"‚úÖ Clasificaci√≥n completada: {len(clasificacion)} singularidades")
    
    # Test de m√©tricas
    print("üîç Testing m√©tricas...")
    metricas = calcular_metricas_calidad_rapidas(singularidades, despertares + dormidas, df_test)
    print(f"‚úÖ M√©tricas calculadas: F1={metricas['f1_score']:.3f}")
    
    print("üéâ Test del Modelo PAH* completado exitosamente!")
    return True
# ===============================================
# üéØ FUNCI√ìN DE UTILIDAD PARA DEBUGGING
# ===============================================

def debug_archivo_individual(archivo_psg, archivo_hypno):
    """Debug de un archivo individual para troubleshooting"""
    print(f"\nüîç DEBUG: Analizando archivo individual")
    print(f"PSG: {archivo_psg}")
    print(f"Hypnogram: {archivo_hypno}")
    
    try:
        # Test de carga
        print("üì• Testing carga de EEG...")
        eeg_data = cargar_eeg_optimizado(archivo_psg, TAMANO_VENTANA)
        print(f"‚úÖ EEG cargado: {len(eeg_data)} ventanas")
        
        # Test de hypnogram
        print("üìã Testing hypnogram...")
        df_hypno = procesar_hypnogram_edf_directo(archivo_hypno)
        if df_hypno is not None:
            print(f"‚úÖ Hypnogram procesado: {len(df_hypno)} ventanas")
        else:
            print("‚ùå Error procesando hypnogram")
            return False
        
        # Test de alineaci√≥n
        n_ventanas = min(len(eeg_data), len(df_hypno))
        print(f"üîó Alineaci√≥n: {n_ventanas} ventanas")
        
        # Test de m√©tricas (solo primeras 10 ventanas)
        print("üßÆ Testing m√©tricas (muestra)...")
        df_muestra = df_hypno.iloc[:10].copy()
        eeg_muestra = eeg_data[:10]
        
        df_con_metricas = calcular_metricas_batch_optimizado(df_muestra, eeg_muestra, batch_size=5)
        
        print("‚úÖ Debug completado exitosamente")
        return True
        
    except Exception as e:
        print(f"‚ùå Error en debug: {e}")
        return False

# ===============================================
# üöÄ FUNCI√ìN PRINCIPAL EXTENDIDA
# ===============================================

def main_con_verificaciones():
    """Funci√≥n principal con verificaciones completas"""
    print("üöÄ Iniciando Modelo PAH* V2.1 con verificaciones completas...")
    
    # 1. Verificar dependencias
    if not verificar_dependencias():
        return
    
    # 2. Test b√°sico
    if not test_modelo_pah():
        print("‚ùå El test b√°sico fall√≥")
        return
    
    # 3. Ejecutar pipeline principal
    main_optimizado()

# ===============================================
# üìã INSTRUCCIONES DE USO
# ===============================================
"""
üöÄ MODELO PAH* V2.1 - OPTIMIZADO PARA i7-11800H

INSTRUCCIONES DE USO:
1. Aseg√∫rate de tener los archivos EDF en la carpeta especificada
2. Ejecuta: python nombre_archivo.py
3. El programa mostrar√° el progreso en tiempo real
4. Si se interrumpe, puedes continuar ejecutando el mismo comando
5. Los resultados se guardan en EUREKA_OPTIMIZED_i7/

CARACTER√çSTICAS:
‚úÖ Optimizado para i7-11800H (8C/16T)
‚úÖ Gesti√≥n inteligente de memoria (32GB RAM)
‚úÖ Sistema de checkpoints autom√°ticos
‚úÖ Progreso persistente con ETA
‚úÖ Sin curvatura de Ricci (optimizaci√≥n de velocidad)
‚úÖ Procesamiento vectorizado
‚úÖ Monitoreo de recursos en tiempo real

DESARROLLADO POR: Camilo Alejandro Sj√∂berg Tala
MODELO: PAH* - Detecci√≥n del Horizonte H* y Pliegue Autops√≠quico
VERSI√ìN: V2.1 Optimizada
"""